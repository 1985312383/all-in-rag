import{_ as i}from"./1_1_2-DaRHvR2c.js";import{_ as n,c as o,a as t,e as r,r as s,o as l}from"./app-CFXp0Idr.js";const c={};function d(g,e){const a=s("Mermaid");return l(),o("div",null,[e[0]||(e[0]=t('<h2 id="_1-what-is-rag" tabindex="-1"><a class="header-anchor" href="#_1-what-is-rag"><span>1. What is RAG?</span></a></h2><h3 id="_1-1-core-definition" tabindex="-1"><a class="header-anchor" href="#_1-1-core-definition"><span>1.1 Core Definition</span></a></h3><p>RAG (Retrieval-Augmented Generation) is a technical paradigm that <strong>combines information retrieval with text generation</strong>. Its core logic is: before a large language model (LLM) generates text, it first dynamically retrieves relevant information from external knowledge bases through a retrieval mechanism, and integrates the retrieval results into the generation process, thereby improving the accuracy and timeliness of the output<sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup><sup class="footnote-ref"><a href="#footnote2">[2]</a><a class="footnote-anchor" id="footnote-ref2"></a></sup><sup class="footnote-ref"><a href="#footnote3">[3]</a><a class="footnote-anchor" id="footnote-ref3"></a></sup>.</p><blockquote><p>Of course, the definition of RAG will expand with technological development, so the current definition serves only as the establishment of a basic framework.</p></blockquote><p>üí° <strong>RAG Essence</strong>: Before LLM generates text, first retrieve relevant information from <strong>external knowledge bases</strong> as context to assist in generating more accurate answers.</p><h3 id="_1-2-technical-principles" tabindex="-1"><a class="header-anchor" href="#_1-2-technical-principles"><span>1.2 Technical Principles</span></a></h3><ul><li><strong>Two-Stage Architecture</strong>:</li></ul>',7)),r(a,{code:"eJx1j71OwzAUhfc+xVVYYKgAVZEqBqTmpwykA0VMVgc3OU4ijF3ZDqVvT+r+xIj2Dpal79M59wqpt2XDjaNiOaJ+ZuzDwtBbB7Nb0Xj8TAlbwpkW31zSQledxMqb/rHdujZ809DZYdGgvzteIzro+0l8YMpu8x8Ho3rjVemtRFWDEm5xN6ipV7O+W/ZRylGmy+4Lytn7tOnUpz2oUNWFZV6gYLhrtWLR8P+3TuY7chYo4YH7yb0yZ0Wx6E+0G63skf5tdjsJmpFopXy6waOIBQKQnMBUxJgGID0CIcQEDwHITmCCWMQByK9Fzc9RZbkuR784dYoQ"}),e[1]||(e[1]=t('<ul><li><strong>Key Components</strong>: <ol><li><strong>Indexing</strong> üìë: Split unstructured documents (PDF/Word, etc.) into chunks and convert them into vector data through embedding models.</li><li><strong>Retrieval</strong> üîçÔ∏è: Based on query semantics, recall the most relevant document chunks (Context) from the vector database.</li><li><strong>Generation</strong> ‚ú®: Use retrieval results as context input to LLM to generate natural language responses.</li></ol></li></ul><h3 id="_1-3-technical-evolution-classification" tabindex="-1"><a class="header-anchor" href="#_1-3-technical-evolution-classification"><span>1.3 Technical Evolution Classification</span></a></h3><p>RAG technology can be classified by complexity<sup class="footnote-ref"><a href="#footnote4">[4]</a><a class="footnote-anchor" id="footnote-ref4"></a></sup>:</p><p><strong>Basic RAG</strong></p><ul><li>Basic &quot;indexing-retrieval-generation&quot; workflow</li><li>Simple document chunking</li><li>Basic vector retrieval mechanism</li></ul><p><strong>Advanced RAG</strong></p><ul><li>Added data cleaning processes</li><li>Metadata optimization</li><li>Multi-round retrieval strategies</li><li>Improved accuracy and efficiency</li></ul><p><strong>Modular RAG</strong></p><ul><li>Flexible integration with search engines</li><li>Reinforcement learning optimization</li><li>Knowledge graph enhancement</li><li>Support for complex business scenarios</li></ul><figure><img src="'+i+`" alt="Classification Diagram" tabindex="0" loading="lazy"><figcaption>Classification Diagram</figcaption></figure><h2 id="_2-why-use-rag" tabindex="-1"><a class="header-anchor" href="#_2-why-use-rag"><span>2. Why Use RAG<sup class="footnote-ref"><a href="#footnote5">[5]</a><a class="footnote-anchor" id="footnote-ref5"></a></sup>?</span></a></h2><h3 id="_2-1-solving-core-limitations-of-llms" tabindex="-1"><a class="header-anchor" href="#_2-1-solving-core-limitations-of-llms"><span>2.1 Solving Core Limitations of LLMs</span></a></h3><table><thead><tr><th>Problem</th><th>RAG Solution</th></tr></thead><tbody><tr><td><strong>Static Knowledge Limitations</strong></td><td>Real-time retrieval from external knowledge bases, supporting dynamic updates</td></tr><tr><td><strong>Hallucination</strong></td><td>Generate based on retrieved content, reducing error rates</td></tr><tr><td><strong>Insufficient Domain Expertise</strong></td><td>Introduce domain-specific knowledge bases (e.g., medical/legal)</td></tr><tr><td><strong>Data Privacy Risks</strong></td><td>Local deployment of knowledge bases, avoiding sensitive data leakage</td></tr></tbody></table><h3 id="_2-2-key-advantages" tabindex="-1"><a class="header-anchor" href="#_2-2-key-advantages"><span>2.2 Key Advantages</span></a></h3><ol><li><strong>Accuracy Improvement</strong></li></ol><ul><li>Knowledge base expansion: Supplement the deficiencies of LLM pre-training knowledge, enhance understanding of professional domains</li><li>Reduce hallucination phenomena: Provide specific reference materials to reduce fabricated information</li><li>Traceable citations: Support citing original documents, improving credibility and persuasiveness of output content</li></ul><ol start="2"><li><strong>Real-time Guarantee</strong></li></ol><ul><li>Dynamic knowledge updates: Knowledge base content can be updated and maintained in real-time independently of the model</li><li>Reduce time lag: Avoid knowledge timeliness issues caused by LLM pre-training data cutoff dates</li></ul><ol start="3"><li><strong>Cost Effectiveness</strong></li></ol><ul><li>Avoid frequent fine-tuning: Compared to repeatedly fine-tuning LLMs, maintaining knowledge bases is more cost-effective</li><li>Reduce inference costs: For domain-specific problems, smaller base models can be used with knowledge bases</li><li>Resource consumption optimization: Reduce computational resource requirements for storing complete knowledge in model weights</li><li>Quick adaptation to changes: New information or policy updates only require updating the knowledge base, no model retraining needed</li></ul><ol start="4"><li><strong>Scalability</strong></li></ol><ul><li>Multi-source integration: Support building unified knowledge bases from different sources and formats of data</li><li>Modular design: Retrieval components can be optimized independently without affecting generation components</li></ul><h3 id="_2-3-risk-graded-application-scenarios" tabindex="-1"><a class="header-anchor" href="#_2-3-risk-graded-application-scenarios"><span>2.3 Risk-Graded Application Scenarios</span></a></h3><blockquote><p>The following shows the applicability of RAG technology in scenarios with different risk levels</p></blockquote><table><thead><tr><th style="text-align:center;">Risk Level</th><th style="text-align:left;">Examples</th><th style="text-align:center;">RAG Applicability</th></tr></thead><tbody><tr><td style="text-align:center;"><strong>Low Risk</strong></td><td style="text-align:left;">Translation/Grammar checking</td><td style="text-align:center;">High reliability</td></tr><tr><td style="text-align:center;"><strong>Medium Risk</strong></td><td style="text-align:left;">Contract drafting/Legal consultation</td><td style="text-align:center;">Requires human review</td></tr><tr><td style="text-align:center;"><strong>High Risk</strong></td><td style="text-align:left;">Evidence analysis/Visa decisions</td><td style="text-align:center;">Requires strict quality control mechanisms</td></tr></tbody></table><h2 id="_3-how-to-get-started-with-rag" tabindex="-1"><a class="header-anchor" href="#_3-how-to-get-started-with-rag"><span>3. How to Get Started with RAG?</span></a></h2><h3 id="_3-1-basic-toolchain-selection" tabindex="-1"><a class="header-anchor" href="#_3-1-basic-toolchain-selection"><span>3.1 Basic Toolchain Selection</span></a></h3><p><strong>Development Frameworks</strong></p><ul><li><strong>LangChain</strong>: Provides pre-built RAG chains (like rag_chain), supporting quick integration of LLM with vector databases</li><li><strong>LlamaIndex</strong>: Optimized for knowledge base indexing, simplifying document chunking and embedding processes</li></ul><p><strong>Vector Databases</strong></p><ul><li><strong>Milvus</strong>: Open-source high-performance vector database</li><li><strong>FAISS</strong>: Lightweight vector search library</li><li><strong>Pinecone</strong>: Cloud service vector database</li></ul><h3 id="_3-2-four-steps-to-build-minimum-viable-product-mvp" tabindex="-1"><a class="header-anchor" href="#_3-2-four-steps-to-build-minimum-viable-product-mvp"><span>3.2 Four Steps to Build Minimum Viable Product (MVP)</span></a></h3><ol><li><p><strong>Data Preparation</strong></p><ul><li>Format support: PDF, Word, web text, etc.</li><li>Chunking strategy: Split by semantics (like paragraphs) or fixed length, avoiding information fragmentation</li></ul></li><li><p><strong>Index Construction</strong></p><ul><li>Embedding models: Choose open-source models (like text-embedding-ada-002) or fine-tune domain-specific models</li><li>Vectorization: Convert text chunks to vectors and store in database</li></ul></li><li><p><strong>Retrieval Optimization</strong></p><ul><li>Hybrid retrieval: Combine keyword (BM25) with semantic search (vector similarity) to improve recall</li><li>Reranking: Use small models to filter Top-K relevant chunks (like Cohere Reranker)</li></ul></li><li><p><strong>Generation Integration</strong></p><ul><li>Prompt engineering: Design templates to guide LLM in integrating retrieved content</li><li>LLM selection: GPT, Claude, Ollama, etc. (balance cost/performance trade-offs)</li></ul></li></ol><h3 id="_3-3-beginner-friendly-solutions" tabindex="-1"><a class="header-anchor" href="#_3-3-beginner-friendly-solutions"><span>3.3 Beginner-Friendly Solutions</span></a></h3><ul><li><strong>LangChain4j Easy RAG</strong>: Simply upload documents, automatically handle indexing and retrieval</li><li><strong>FastGPT</strong>: Open-source knowledge base platform with visual RAG workflow configuration</li><li><strong>GitHub Templates</strong>: Such as &quot;TinyRAG&quot; project<sup class="footnote-ref"><a href="#footnote6">[6]</a><a class="footnote-anchor" id="footnote-ref6"></a></sup>, providing complete code</li></ul><h3 id="_3-4-advanced-optimization-directions" tabindex="-1"><a class="header-anchor" href="#_3-4-advanced-optimization-directions"><span>3.4 Advanced Optimization Directions</span></a></h3><p><strong>Evaluation Metrics</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-"><span class="line"><span>Retrieval Quality: Context Relevance</span></span>
<span class="line"><span>Generation Quality: Faithfulness, Factual Accuracy</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Performance Optimization</strong></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-"><span class="line"><span>Layered Indexing: Enable caching mechanisms for high-frequency data</span></span>
<span class="line"><span>Multimodal Extension: Support image/table retrieval</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>RAG technology is still rapidly developing, so keep following the latest advances in academia and industry!</p></blockquote><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="footnote1" class="footnote-item"><p><a href="https://www.researchgate.net/publication/391141346_Retrieval-Augmented_Generation_Methods_Applications_and_Challenges" target="_blank" rel="noopener noreferrer">Genesis, J. (2025). <em>Retrieval-Augmented Text Generation: Methods, Challenges, and Applications</em></a>. <a href="#footnote-ref1" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="footnote2" class="footnote-item"><p><a href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener noreferrer">Gao et al. (2023). <em>Retrieval-Augmented Generation for Large Language Models: A Survey</em></a>. <a href="#footnote-ref2" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="footnote3" class="footnote-item"><p><a href="https://arxiv.org/abs/2005.11401" target="_blank" rel="noopener noreferrer">Lewis et al. (2020). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em></a>. <a href="#footnote-ref3" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="footnote4" class="footnote-item"><p><a href="https://arxiv.org/abs/2407.21059" target="_blank" rel="noopener noreferrer">Gao et al. (2024). <em>Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks</em></a>. <a href="#footnote-ref4" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="footnote5" class="footnote-item"><p><a href="https://www.lawdroidmanifesto.com/p/rag-why-does-it-matter-what-is-it" target="_blank" rel="noopener noreferrer"><em>RAG: Why Does It Matter, What Is It, and Does It Guarantee Accuracy?</em></a>. <a href="#footnote-ref5" class="footnote-backref">‚Ü©Ô∏é</a></p></li><li id="footnote6" class="footnote-item"><p><a href="https://github.com/KMnO4-zx/TinyRAG" target="_blank" rel="noopener noreferrer"><em>TinyRAG: GitHub Project</em></a>. <a href="#footnote-ref6" class="footnote-backref">‚Ü©Ô∏é</a></p></li></ol></section>`,44))])}const u=n(c,[["render",d]]),f=JSON.parse('{"path":"/en/chapter1/1.RAG_intro.html","title":"Chapter 1: Introduction to RAG","lang":"en-US","frontmatter":{"createTime":"2025/09/29 15:04:14","title":"Chapter 1: Introduction to RAG"},"readingTime":{"minutes":3.19,"words":958},"git":{"createdTime":1755578909000,"updatedTime":1759130705000,"contributors":[{"name":"FutureUnreal","username":"FutureUnreal","email":"42101210307@stu.xpu.edu.cn","commits":6,"avatar":"https://avatars.githubusercontent.com/FutureUnreal?v=4","url":"https://github.com/FutureUnreal"},{"name":"1985312383","username":"1985312383","email":"56398475+1985312383@users.noreply.github.com","commits":2,"avatar":"https://avatars.githubusercontent.com/1985312383?v=4","url":"https://github.com/1985312383"}]},"filePathRelative":"en/chapter1/1.RAG_intro.md","headers":[]}');export{u as comp,f as data};
