### 向量嵌入到RAG嵌入技术的发展：演进路径与技术融合

#### **一、向量嵌入的起源与早期发展（2000-2013年）**

1. **理论基础与初步尝试**

   * 向量嵌入的核心思想是将离散符号（如单词）映射为连续向量空间中的点，以捕捉语义关系。其雏形可追溯至2000年Bengio等人的 **神经网络语言模型（NNLM）** ，首次将单词表示为低维向量。
   * 早期模型（如NNLM）受限于计算能力和数据规模，仅能处理小规模语料，且未充分捕捉语义关联。

2. **Word2Vec的突破（2013年）**

   * Mikolov等人提出的**Word2Vec**成为里程碑式突破：

* 通过**Skip-gram**和**CBOW**两种架构，利用局部上下文窗口学习词向量。
* 创新性地引入 **负采样（Negative Sampling）** 和 **子采样（Subsampling）** ，显著提升训练效率和向量质量。
* 首次验证了向量运算的语义能力（如“国王 - 男人 + 女人 ≈ 王后”）。

3. **GloVe的全局优化（2014年）**
   * Pennington等人的**GloVe模型**融合了全局矩阵分解与局部上下文方法的优势：

* 基于词-词共现矩阵的全局统计信息，捕捉长距离语义依赖。
* 在词类比任务上达到75%准确率，超越同期模型。

> **技术意义**：Word2Vec和GloVe奠定了**静态词嵌入**的基础，但局限性明显：
>
> * 仅生成上下文无关的固定向量，无法处理多义词（如“苹果”既指水果也指公司）。
> * 无法直接表示短语或长文本（如“Air Canada”需特殊处理）。

***

#### **二、嵌入技术的进阶：从词到多粒度文本（2014-2018年）**

1. **短语与句子嵌入的探索**

   * Word2Vec的扩展支持**短语嵌入**，通过识别固定搭配（如“New York”）提升复合语义表示。
   * **Doc2Vec**等模型尝试生成文档级向量，但效果受限于浅层架构。

2. **Transformer革命（2017年）**

   * Vaswani等人的**Transformer架构**引入自注意力机制：

* 动态生成上下文相关向量（如BERT的token嵌入）。
* 为后续预训练模型提供基础架构，支持长文本编码。

3. **预训练模型的崛起（2018-2019年）**
   * **BERT**（Bidirectional Encoder Representations）通过掩码语言建模，生成深度上下文嵌入：

* 同一词在不同语境中生成不同向量，解决多义性问题。
* 可输出词、句、段等多粒度嵌入。
  * **GPT**系列则通过自回归生成任务优化文本表示。

> **技术跃迁**：
>
> * 嵌入对象从**词**扩展至**句子/文档**，支持更大语义单元。
> * 动态嵌入取代静态嵌入，成为NLP任务的新标准。

***

#### **三、RAG的诞生与嵌入技术的核心作用（2020年至今）**

1. **RAG的提出背景**

   * 大型语言模型（LLM）存在**知识固化**（训练后无法更新知识）和**幻觉**（生成不实内容）问题。
   * 2020年Facebook团队发表奠基性论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》，首次提出**RAG框架**：

* **核心思想**：将外部知识检索与LLM生成结合，动态注入实时信息。
* **嵌入技术的作用**：将知识库文档和查询转换为向量，实现语义检索。

2. **RAG的工作流程与嵌入技术**\
   RAG分为三阶段，嵌入技术贯穿始终：
   * **索引阶段**：

* 文档分割为语义块（Chunk），通过嵌入模型（如BERT）转换为向量。
* 向量存储于数据库（如FAISS），建立高效检索索引。
  * **检索阶段**：
* 用户查询被嵌入为向量，通过**相似度计算**（如余弦相似度）匹配知识库中的相关文档块。
* 嵌入质量直接决定检索精度。
  * **生成阶段**：
* 检索结果与查询拼接，输入LLM生成最终回答。

3. **RAG对嵌入技术的革新要求**

   * **多模态支持**：需处理文本、代码、图像等混合数据（如医疗RAG系统需整合医学图像与报告）。
   * **领域自适应**：专业领域（如法律、医疗）需微调嵌入模型，适应术语特征。
   * **效率优化**：

* 嵌入维度需平衡语义丰富度与计算开销（高维更精准但消耗资源）。
* 轻量化模型（如INSTRUCTOR）支持实时检索。
  * **混合检索机制**：
* 结合密集向量（语义匹配）与稀疏向量（关键词匹配），提升召回率。

***

#### **四、关键技术突破与框架演进**

1. **嵌入模型的专用化**
   * **指令微调嵌入模型**：

* INSTRUCTOR模型通过任务指令生成领域自适应嵌入，在70+任务中达到SOTA。
* 例如：“检索医学文献”指令使嵌入聚焦专业术语。
  * **多语言嵌入**：
* 跨语言模型（如LaBSE）支持非英语RAG系统，解决语义对齐问题。

接下来是两个老师的课程录音

老师1：
嵌入技术的基本知识：
1
00:00:00,000 --> 00:00:01,068
向量嵌入技术

2
00:00:01,070 --> 00:00:05,570
向量嵌入技术是现代人工智能系统的知识内核

3
00:00:05,580 --> 00:00:08,557
那它当然是一个很重要的章节

4
00:00:08,565 --> 00:00:14,821
向量嵌入技术是在我们做了文档的导入和分块之后的

5
00:00:14,821 --> 00:00:19,055
第三个重要技术在这个部位

6
00:00:19,055 --> 00:00:25,919
那么向量嵌入实际上它有几个环节需要我们去领悟

7
00:00:25,920 --> 00:00:29,294
第一个就是向量嵌入模型的选择

8
00:00:29,300 --> 00:00:30,764
这个当然是很重要

9
00:00:30,770 --> 00:00:35,218
因为基本上来说我们向量嵌入的选择

10
00:00:35,218 --> 00:00:41,990
就确定了我们的知识库和RAG检索系统的能力上限

11
00:00:41,990 --> 00:00:44,915
如果你都没有一个良好的知识表示

12
00:00:44,925 --> 00:00:49,245
那我们进一步的优质的检索也就无从谈起

13
00:00:49,260 --> 00:00:54,972
所有的后续的优化过程都是在这个上限基础之上的优化

14
00:00:54,980 --> 00:00:57,763
你没有一个好的嵌入模型

15
00:00:57,763 --> 00:01:02,694
所以你再怎么做重排再怎么做更好的检索和召回

16
00:01:02,695 --> 00:01:05,121
做生成做数据的切块

17
00:01:05,125 --> 00:01:09,970
你都只是在一个很低的上限底下做文章

18
00:01:09,970 --> 00:01:13,008
那如果我这么讲这个上限两个字

19
00:01:13,015 --> 00:01:16,011
大家就应该知道嵌入模型的选择

20
00:01:16,011 --> 00:01:19,184
在咱们整个的RAG系统中有多重要

21
00:01:19,185 --> 00:01:21,765
所以说通常我做项目的时候

22
00:01:21,765 --> 00:01:24,824
我会首先用OpenAI的模型去做实验

23
00:01:24,825 --> 00:01:27,177
因为这个代表着一个业界的标杆

24
00:01:29,300 --> 00:01:32,480
我只是努力的去接近它而已

25
00:01:32,480 --> 00:01:34,070
目前我的体会是这样的

26
00:01:34,070 --> 00:01:36,454
如果大家特定领域有特定的嵌入模型

27
00:01:36,465 --> 00:01:38,745
你觉得比OpenAI的嵌入模型好

28
00:01:38,750 --> 00:01:39,942
那你一定要告诉我

29
00:01:39,945 --> 00:01:42,033
我们看看最好的模型是什么

30
00:01:42,040 --> 00:01:45,040
也许是Google的Gemini系列的嵌入模型

31
00:01:45,045 --> 00:01:47,895
也许是什么其他的公司的商业模型

32
00:01:47,905 --> 00:01:49,673
但是在开源的领域

33
00:01:49,673 --> 00:01:52,325
你很难找到说一个嵌入模型

34
00:01:52,325 --> 00:01:55,360
会超过商业付费的模型

35
00:01:55,360 --> 00:01:58,415
那也许我这句话说的有点绝对

36
00:01:58,415 --> 00:02:01,223
但是大家如果有好的模型

37
00:02:01,223 --> 00:02:03,095
可以相互的去交流

38
00:02:03,100 --> 00:02:07,015
那么嵌入技术还有中间的这个位置

39
00:02:07,020 --> 00:02:09,517
就是稀疏嵌入和密集嵌入

40
00:02:09,520 --> 00:02:13,888
还有这个涉及到后续我们可以生成多表示的索引

41
00:02:13,890 --> 00:02:17,245
然后我们可以利用这些稀疏嵌入、密集嵌入

42
00:02:17,245 --> 00:02:20,353
各种各样的嵌入方式做混合检索

43
00:02:20,365 --> 00:02:22,092
所以这是一个重要知识点

44
00:02:22,095 --> 00:02:27,535
当然我们还有一些特殊的嵌入模型

45
00:02:27,545 --> 00:02:30,053
这些特殊的嵌入模型

46
00:02:30,053 --> 00:02:33,625
是可以基于特定领域进行微调的模型

47
00:02:33,625 --> 00:02:36,442
好那么我们这节课

48
00:02:36,442 --> 00:02:39,071
我把它分成七部分来讲解

49
00:02:39,085 --> 00:02:41,365
首先是嵌入技术的基本知识

50
00:02:41,365 --> 00:02:43,645
然后是它的发展和演变过程

51
00:02:43,665 --> 00:02:45,535
那么重点我们当然是要谈

52
00:02:45,535 --> 00:02:47,915
我们目前大模型时代的嵌入技术

53
00:02:47,930 --> 00:02:51,486
都有哪些好的公司商用的也好

54
00:02:51,495 --> 00:02:53,833
开源的也好现在我们都选择了

55
00:02:53,833 --> 00:02:55,343
什么样的一些嵌入模型

56
00:02:55,360 --> 00:02:57,040
中文的也好英文的也好

57
00:02:57,045 --> 00:02:58,995
然后我们谈稀疏嵌入密集嵌入

58
00:02:58,995 --> 00:03:01,075
模型嵌入和混合嵌入的实现

59
00:03:01,075 --> 00:03:02,855
这块我们会有代码实战

60
00:03:02,855 --> 00:03:05,345
然后多模态嵌入技术的应用和实战

61
00:03:05,345 --> 00:03:06,815
这块我们会有代码实战

62
00:03:06,820 --> 00:03:08,800
然后我们再简单的谈一下

63
00:03:08,800 --> 00:03:11,320
嵌入技术的一些最新的发展方向

64
00:03:11,320 --> 00:03:13,310
那这个进化是比较快的

65
00:03:13,310 --> 00:03:16,494
更多的就需要大家去基于这些知识点

66
00:03:16,494 --> 00:03:17,898
自己去做探索

67
00:03:19,360 --> 00:03:22,025
好首先来看嵌入的基本的知识

68
00:03:22,030 --> 00:03:24,351
我们首先理解什么是嵌入

69
00:03:24,360 --> 00:03:26,614
就今天的大模型为什么这么厉害

70
00:03:26,615 --> 00:03:29,219
实际上它就是通过嵌入技术

71
00:03:29,225 --> 00:03:32,165
它才能够把世界上所有的这些知识

72
00:03:32,165 --> 00:03:36,269
语言文字甚至图片语音这些多模态的东西

73
00:03:36,269 --> 00:03:38,093
全部都收纳在自己

74
00:03:38,100 --> 00:03:39,964
的内部自己的脑海

75
00:03:39,964 --> 00:03:43,459
形成了一个无比巨大的一个知识库

76
00:03:43,465 --> 00:03:45,027
你可以把它看成是知识库

77
00:03:45,027 --> 00:03:46,305
可以把它看成图书馆

78
00:03:46,305 --> 00:03:48,758
或者说一个神秘的水晶球

79
00:03:48,758 --> 00:03:50,988
它里边装的全都是向量

80
00:03:51,005 --> 00:03:54,035
就代表了我们整个世界的知识内涵

81
00:03:54,045 --> 00:03:57,030
我们人类所能够数字化的所有信息

82
00:03:57,035 --> 00:03:59,193
图书也好博客也好音视频也好

83
00:03:59,200 --> 00:04:00,880
都是在大模型里面

84
00:04:00,885 --> 00:04:03,305
我们之所以能跟它去交流

85
00:04:03,305 --> 00:04:06,165
都是拜这个向量嵌入技术之赐

86
00:04:06,185 --> 00:04:08,125
所以我们如果要去里边

87
00:04:08,125 --> 00:04:10,453
打开这个知识魔盒去检索它

88
00:04:10,465 --> 00:04:14,001
那么一样你也需要对你的这个查询文本

89
00:04:14,001 --> 00:04:18,053
做同样的嵌入你才能够去接触它的世界

90
00:04:18,055 --> 00:04:21,639
也就是说DeepSeek它有一批这个向量嵌入

91
00:04:21,645 --> 00:04:23,389
你要去跟它做对话

92
00:04:23,389 --> 00:04:26,224
那么DeepSeek需要把你的这个段话

93
00:04:26,224 --> 00:04:28,059
用它一样的嵌入方式

94
00:04:28,059 --> 00:04:30,620
然后去和它里边的知识信息

95
00:04:30,620 --> 00:04:32,224
做这个相似度的比较

96
00:04:32,224 --> 00:04:34,338
然后才能够检索出来有用的信息

97
00:04:34,345 --> 00:04:36,182
所以在群里边前一段时间

98
00:04:36,182 --> 00:04:37,518
大家有同学探讨说

99
00:04:37,535 --> 00:04:41,390
哎呀我已经做了知识库的嵌入系统

100
00:04:41,395 --> 00:04:45,803
然后我忘了我是用什么这个嵌入模型做的了

101
00:04:45,805 --> 00:04:47,845
现在我怎么检索都检索不对

102
00:04:47,850 --> 00:04:50,215
当然那个维度也匹配不上

103
00:04:50,215 --> 00:04:53,439
你一定要用你所做的嵌入模型

104
00:04:53,439 --> 00:04:56,347
来去对你的查询也好问题也好

105
00:04:56,347 --> 00:04:58,867
你的一小段知识也好做相同的嵌入

106
00:04:58,880 --> 00:05:02,015
然后再去你的这个知识库里面去检索做比较

107
00:05:02,030 --> 00:05:06,038
这是一个基本的出发点好

108
00:05:06,038 --> 00:05:08,954
那么你可以把它类比成人脑

109
00:05:08,954 --> 00:05:12,553
就是AI知识库是怎么样获得知识的

110
00:05:12,553 --> 00:05:14,629
和我们人类的这个感知系统

111
00:05:14,629 --> 00:05:16,013
是怎样获得知识的

112
00:05:16,015 --> 00:05:19,910
它都是一个非常类似的知识获得的过程

113
00:05:19,910 --> 00:05:24,140
它不外乎就是把外界这些非结构化的东西

114
00:05:24,140 --> 00:05:26,255
给它用某种手段内化

115
00:05:26,255 --> 00:05:28,859
那么我们人类大脑一定也能够用这种手段内化

116
00:05:28,875 --> 00:05:31,675
经过了类似的处理过程

117
00:05:31,675 --> 00:05:34,927
所以归根结底一切都是数学

118
00:05:34,930 --> 00:05:37,180
感官信号到达大脑之后

119
00:05:37,180 --> 00:05:41,230
大脑一定会把这些信号通过神经脉冲

120
00:05:41,230 --> 00:05:43,580
转化成一系列的0101

121
00:05:43,580 --> 00:05:45,550
因为神经脉冲也是高低

122
00:05:45,550 --> 00:05:48,111
不外乎就是电平里面的01数字

123
00:05:48,120 --> 00:05:53,664
然后去根据这些神经元的脉冲频率模式进行编码

124
00:05:53,670 --> 00:05:56,020
后边也是一系列的嵌入

125
00:05:56,025 --> 00:06:00,624
实际上就是把这些信息通过大脑内部的神经编码

126
00:06:00,625 --> 00:06:03,013
大脑里面的神经网络的编码

127
00:06:03,015 --> 00:06:06,330
然后给它转换成了一系列的电信号

128
00:06:06,330 --> 00:06:11,192
然后在我们的这个记忆内存里面形成了一些东西

129
00:06:11,210 --> 00:06:14,769
然后在大脑里面保存了这个保存的过程

130
00:06:14,769 --> 00:06:17,870
那就是下边一个章节这个向量数据库

131
00:06:17,870 --> 00:06:19,430
相当于是一个存储

132
00:06:19,435 --> 00:06:21,085
现在我们还没有谈到存储

133
00:06:21,090 --> 00:06:24,180
我们现在只是谈怎么转信号的过程

134
00:06:24,185 --> 00:06:29,258
所以说嵌入就是把外界信息通过学习的方法

135
00:06:29,258 --> 00:06:32,319
通过各种各样的模型和手段

136
00:06:32,319 --> 00:06:35,514
转换成一种向量表示一种神经脉冲

137
00:06:35,514 --> 00:06:37,431
一种数字信号的过程

138
00:06:37,435 --> 00:06:39,547
所以这是一个其实非常好的一个类比

139
00:06:39,555 --> 00:06:44,231
那么我们计算机世界里面的向量它其实更整齐

140
00:06:44,235 --> 00:06:48,835
它比我们大脑里面生成的那些神经信号更明确

141
00:06:48,845 --> 00:06:50,405
你可以拷贝来拷贝去的

142
00:06:50,405 --> 00:06:52,425
它是一系列固定的数字

143
00:06:52,425 --> 00:06:55,225
不像我们大脑里面产生的东西很模糊

144
00:06:55,235 --> 00:06:57,770
你知道是一系列的神经元神经信号

145
00:06:57,770 --> 00:06:59,810
你也不知道它怎么保存怎么处理的

146
00:06:59,810 --> 00:07:00,898
数学上怎么计算的

147
00:07:00,910 --> 00:07:01,675
这个太难了

148
00:07:01,675 --> 00:07:04,879
很难把一个人的脑袋割开去了解看这些事

149
00:07:04,880 --> 00:07:07,519
所以有一个一系列的思想思潮

150
00:07:07,519 --> 00:07:10,767
就是我们有了这个现在现在神经网络

151
00:07:10,767 --> 00:07:14,524
和这个大圆模型其实我们可以通过研究大圆模型

152
00:07:14,524 --> 00:07:17,675
反过来研究这样不可知的人脑对吧

153
00:07:17,675 --> 00:07:22,100
这也是一个思路但是向量它就是有维度有数字

154
00:07:22,100 --> 00:07:25,831
有清晰的表示你可以把这个向量拷贝来拷贝去

155
00:07:25,835 --> 00:07:30,490
但是每一个大圆模型也好BGE也好

156
00:07:30,490 --> 00:07:33,946
Harding Face上面的一系列的线路向量模型也好

157
00:07:33,950 --> 00:07:38,129
他们的这个学习向量的方式都是有一定的区别的

158
00:07:38,140 --> 00:07:43,099
他们最后生成的对同一段文字所生成的向量

159
00:07:43,099 --> 00:07:47,436
从它的数字本身和维度大小都是不一样的

160
00:07:47,440 --> 00:07:49,705
那么这个维度呢就是我们通常说的

161
00:07:49,710 --> 00:07:53,841
我的这个模型生成的向量的维度是512维

162
00:07:53,845 --> 00:07:56,884
比如说BERT，我的另外一个模型

163
00:07:56,884 --> 00:07:58,874
比如说OpenAI的text-embedding模型

164
00:07:58,875 --> 00:08:03,534
这个模型生成的维度是1382维，举例不一定准确

165
00:08:03,540 --> 00:08:08,118
那么这个就是它所生成的这个数字的维度信息

166
00:08:08,120 --> 00:08:12,854
那么对于同一段话来说，通常你给的维度越大

167
00:08:12,860 --> 00:08:17,249
那么就意味着你能够表达的细节也就越丰富

168
00:08:17,260 --> 00:08:21,572
那么同时也就意味着你的计算复杂度将会增加很多

169
00:08:21,575 --> 00:08:25,535
那如果现在比较大的向量线性模型

170
00:08:25,535 --> 00:08:28,875
说我能够支持8000维的这个向量线性模型

171
00:08:28,875 --> 00:08:32,799
那么它所需要的计算空间是非常非常大的

172
00:08:32,810 --> 00:08:37,895
那么在数学上它就是一系列的空间坐标了

173
00:08:37,905 --> 00:08:42,427
向量会变成我们数学就是欧几里德空间

174
00:08:42,440 --> 00:08:45,392
也就是我们所熟知的这些二维三维四维的

175
00:08:45,392 --> 00:08:49,703
这个直角坐标系里边的数学空间里边的一系列的向量

176
00:08:49,703 --> 00:08:52,615
有方向的点你可以给它理解成点吧

177
00:08:54,520 --> 00:08:58,840
那么如何去计算这个向量和向量之间

178
00:08:58,855 --> 00:09:03,352
它们的相似性呢其实就是计算它们的距离呀

179
00:09:03,355 --> 00:09:06,475
通过在数学上计算两个向量的距离

180
00:09:06,475 --> 00:09:08,555
来去匹配它们的相似度

181
00:09:08,555 --> 00:09:13,060
所以说这一切就是我们现在谈RAG系统的

182
00:09:13,060 --> 00:09:16,630
这个理论根源就是向量相似度的计算

183
00:09:16,635 --> 00:09:18,637
所以这样一讲大家就基本上明白

184
00:09:18,637 --> 00:09:20,210
这个过程是怎么一回事了

185
00:09:20,220 --> 00:09:26,829
那么向量计算相似度它有多种不同的度量方式

186
00:09:26,835 --> 00:09:31,525
那有这个欧式距离这个英文词啊叫欧几里得

187
00:09:31,525 --> 00:09:34,712
欧式距离就直接计算两个点的距离

188
00:09:34,715 --> 00:09:38,493
然后有曼哈顿距离你可以这样去计算

189
00:09:38,500 --> 00:09:41,782
从这个原点出发那这个曼哈顿距离

190
00:09:41,782 --> 00:09:44,022
在我们的语境中很少用

191
00:09:44,035 --> 00:09:49,623
那这个曼哈顿距离在我们谈到文本的相似度

192
00:09:49,630 --> 00:09:52,920
包括我们计算推荐系统里面

193
00:09:52,920 --> 00:09:56,778
两个向量的相似度都很少用曼哈顿距离

194
00:09:56,780 --> 00:09:59,435
那么我们通常用的最多的相似度呢

195
00:09:59,445 --> 00:10:02,301
就是余弦相似度的比较

196
00:10:02,310 --> 00:10:06,769
它是只关注于两个向量的方向

197
00:10:06,769 --> 00:10:11,571
而不介意它们实际的距离是如何

198
00:10:11,580 --> 00:10:16,065
所以说我们在谈到向量相似度

199
00:10:16,065 --> 00:10:21,948
通常在我们做RAG做文本相似度计算的语境中

200
00:10:21,948 --> 00:10:27,305
我们首先要想到实际上我们计算的就是余弦相似度

201
00:10:27,305 --> 00:10:31,880
那就可以了我们很少使用欧式距离

202
00:10:31,880 --> 00:10:36,435
和曼哈顿距离来做这个文本语义相似度的计算

203
00:10:36,435 --> 00:10:39,768
那么最后一个点击这个有的时候

204
00:10:39,768 --> 00:10:42,939
我们还是需要用到第一就是点击

205
00:10:42,939 --> 00:10:46,647
它是代表着两个向量的强度

206
00:10:46,660 --> 00:10:49,960
它是两个向量的方向和强度的结合

207
00:10:49,960 --> 00:10:53,249
所以有的时候比如说推荐系统

208
00:10:53,249 --> 00:10:56,791
你需要就是对这个强度进行一个

209
00:10:56,800 --> 00:10:59,245
表示比如说我喜欢一个物品

210
00:10:59,245 --> 00:11:02,240
非常喜欢里面加上了一些强度的信息的时候

211
00:11:02,245 --> 00:11:05,089
这个时候你可能会选择点击

212
00:11:05,095 --> 00:11:07,877
但是它其实后面也同时意味着

213
00:11:07,877 --> 00:11:10,659
它们两个向量的方向的相似性

214
00:11:10,660 --> 00:11:14,020
总而言之当我们说到这个语义相似的时候

215
00:11:14,035 --> 00:11:17,778
我们第一想到的就是余弦相似度的计算

216
00:11:17,785 --> 00:11:21,125
那么余弦相似度的计算在数学上它是有公式的

217
00:11:21,130 --> 00:11:23,209
下面这段代码给出的就是

218
00:11:23,209 --> 00:11:25,855
我们这个余弦相似度的计算公式

219
00:11:25,875 --> 00:11:27,932
那当然在我们的系统里面

220
00:11:27,932 --> 00:11:31,511
我们都是通过向量数据库本身的检索功能

221
00:11:31,511 --> 00:11:35,109
来为我们进行这个相似度的计算

222
00:11:35,115 --> 00:11:37,095
在有些特定的场景中

223
00:11:37,095 --> 00:11:40,175
它也给出了我们不同的计算方法

224
00:11:40,185 --> 00:11:42,824
那我们可以选择Mahalanobis里面的IP

225
00:11:42,830 --> 00:11:47,780
就代表着你这个时候要通过点击来计算它的相似度

226
00:11:47,800 --> 00:11:52,525
那么在如果说在一批向量已经做了归一化的前提下

227
00:11:52,545 --> 00:11:58,791
那其实点击IP和余弦相似度它们是等价的

228
00:11:58,805 --> 00:12:01,956
这是在向量已经做了归一化的前提之下它们是等价的

229
00:12:01,960 --> 00:12:05,426
而在数学上有的时候你听说点击dot product

230
00:12:05,426 --> 00:12:08,105
有的时候你听说内积这两个词呢

231
00:12:08,105 --> 00:12:11,522
在咱们这个欧几里德空间中也是等价的

232
00:12:11,530 --> 00:12:13,125
那什么是欧几里德空间呢

233
00:12:13,135 --> 00:12:16,008
刚才说了我们普通人讲的这个数学空间

234
00:12:16,015 --> 00:12:18,015
能够接触到的数学空间

235
00:12:18,015 --> 00:12:20,635
咱们所知道的二维这叫坐标系

236
00:12:20,635 --> 00:12:23,318
三维坐标系甚至四维五维空间

237
00:12:23,318 --> 00:12:25,014
都是欧几里德空间

238
00:12:25,025 --> 00:12:28,882
所以说在这个空间中我们谈点击或者谈内积

239
00:12:28,882 --> 00:12:30,390
全部都是一回事

240
00:12:30,390 --> 00:12:33,790
所以我们做这个向量检索呀推荐系统呀

241
00:12:33,800 --> 00:12:36,752
主要我们是聚焦于cosine和IP

242
00:12:36,755 --> 00:12:38,615
这个我们后续还会讲到

243
00:12:38,615 --> 00:12:40,661
大家先理解到这里就好了

244
00:12:40,665 --> 00:12:43,855
好那么向量的下游应用

245
00:12:43,855 --> 00:12:47,683
它主要是有两大具体的方向

246
00:12:47,700 --> 00:12:50,458
就是你在进行两大类任务的时候

247
00:12:50,458 --> 00:12:53,216
都可能会运用到这个向量的计算

248
00:12:53,235 --> 00:12:56,105
第一类任务就是传统的分类模型

249
00:12:56,105 --> 00:12:57,745
预测一个什么东西

250
00:12:57,760 --> 00:13:03,878
比如说我要对一批这个评论进行情感的分类

251
00:13:03,890 --> 00:13:08,468
这个就是一个典型的过去的情感分类任务

252
00:13:08,485 --> 00:13:10,573
在NLP系统中非常常见

253
00:13:10,573 --> 00:13:13,125
就是你给我一批评论数据

254
00:13:13,125 --> 00:13:17,267
然后我把这一批文本全部打上标签

255
00:13:17,275 --> 00:13:19,775
说这个是好评中评差评

256
00:13:19,775 --> 00:13:22,355
这个怎么去做就是要向量

257
00:13:22,355 --> 00:13:25,244
就是我的特征特征

258
00:13:25,245 --> 00:13:30,367
然后分类的结果就是好评中评差评

259
00:13:30,367 --> 00:13:32,303
就是三个分类标签

260
00:13:32,310 --> 00:13:34,578
它可能是二分类也可能是多分类

261
00:13:34,580 --> 00:13:37,170
那么我们是把这个向量作为特征

262
00:13:37,170 --> 00:13:38,885
输入到比如说SVM

263
00:13:38,885 --> 00:13:41,332
输入到神经网络也可以

264
00:13:41,332 --> 00:13:43,521
输入到这个模型比如说BERT

265
00:13:43,521 --> 00:13:46,234
这种比较小的模型里面去

266
00:13:46,234 --> 00:13:49,050
然后通过监督学习的方式

267
00:13:49,055 --> 00:13:52,429
我说这一批向量我都给它做好了

268
00:13:52,435 --> 00:13:53,849
打上了分类标签

269
00:13:53,849 --> 00:13:56,071
然后呢这个模型训练好了

270
00:13:56,080 --> 00:13:57,972
下一批向量我就能够做出

271
00:13:57,972 --> 00:14:01,446
这一批向量它就有了这个

272
00:14:01,446 --> 00:14:03,054
就是说模型训练好了

273
00:14:03,055 --> 00:14:07,748
那过去的所有的NLP任务都是这样训练出来的

274
00:14:07,750 --> 00:14:09,031
它就相对固定

275
00:14:09,031 --> 00:14:11,776
你可以专门比如说给这一批金融的

276
00:14:11,776 --> 00:14:13,680
这个语料库做这批任务

277
00:14:13,680 --> 00:14:17,480
它是专门根据金融训练出来的这个分类器

278
00:14:17,485 --> 00:14:19,741
下一个来了一批医疗的数据

279
00:14:19,741 --> 00:14:21,245
你再重新训练一次

280
00:14:21,260 --> 00:14:25,098
这个就是我们所说的过去这种判别式模型

281
00:14:25,110 --> 00:14:29,025
它试图解决的是每一个具体的问题

282
00:14:29,030 --> 00:14:31,418
相对来说某个特定的数据集

283
00:14:31,418 --> 00:14:33,806
需要一个特定的模型去解决

284
00:14:33,815 --> 00:14:35,663
这是传统的NLP

285
00:14:35,663 --> 00:14:38,435
有很多过去是这样做任务的

286
00:14:38,450 --> 00:14:40,511
现在我们谈的RAG

287
00:14:40,511 --> 00:14:42,572
或者说大模型出来了

288
00:14:42,580 --> 00:14:44,855
结果它就把这批任务给碾压了

289
00:14:44,855 --> 00:14:47,130
就说你不需要再去认认真真的

290
00:14:47,130 --> 00:14:50,108
去打这些标签然后去训练这些模型太麻烦了

291
00:14:58,860 --> 00:15:00,462
大模型里边丢就好了

292
00:15:00,462 --> 00:15:02,420
所以说我们没有那么麻烦

293
00:15:02,430 --> 00:15:04,998
我们只是查找最相似的内容

294
00:15:04,998 --> 00:15:08,445
然后去基于相似度就给它直接出来了

295
00:15:08,445 --> 00:15:11,240
可以回答你而且这个响应速度

296
00:15:11,240 --> 00:15:14,512
基于RAG和现代向量数据库的检索

297
00:15:14,512 --> 00:15:16,321
它可以优化到毫秒级

298
00:15:16,330 --> 00:15:19,654
迅速的找到一个问题的答案

299
00:15:19,660 --> 00:15:22,686
你比如你再问这句话是好评还是差评啊

300
00:15:22,700 --> 00:15:25,040
它只需要去可能在里面检索

301
00:15:25,040 --> 00:15:27,185
和这个好或者差做个检索

302
00:15:27,205 --> 00:15:29,713
你这句话像好话它就回答你好

303
00:15:29,713 --> 00:15:32,225
你这句话像是一个批评它就回答差

304
00:15:32,225 --> 00:15:34,643
所以说它这个全部通用的问题

305
00:15:34,655 --> 00:15:37,827
不管是这种分类好评差评中评

306
00:15:37,830 --> 00:15:39,940
还是说你问它一个知识

307
00:15:39,940 --> 00:15:41,840
我怎么样去学习Python

308
00:15:41,855 --> 00:15:43,703
我怎么样去学习RAG

309
00:15:43,703 --> 00:15:45,935
你只要有问题它都能回答你

310
00:15:45,940 --> 00:15:48,268
所以现代的这个向量的应用

311
00:15:48,268 --> 00:15:50,596
它就把这个领域无限扩展了

312
00:15:50,600 --> 00:15:53,368
这就是为什么我们现在学习RAG的原因

313
00:15:53,370 --> 00:15:56,340
就是现在逐渐变成了检索式的模型

314
00:15:56,340 --> 00:15:58,122
和这个检索式的应用

315
00:15:58,140 --> 00:15:59,877
但是我们也不要忘记

316
00:15:59,877 --> 00:16:02,193
就是在某些精细化的场景中

317
00:16:02,195 --> 00:16:04,220
我的这个判别式模型

318
00:16:04,220 --> 00:16:07,820
和我这个向量的应用价值还是存在的

319
00:16:07,825 --> 00:16:10,449
尤其是你有一大堆专业数据的情况下

320
00:16:10,455 --> 00:16:12,327
这个还是可以使用的

321
00:16:12,330 --> 00:16:14,250
好这就是我所说的

322
00:16:14,250 --> 00:16:18,020
所谓向量技术的最基本的知识就是这么多

嵌入技术的发展和演变：
1
00:00:02,950 --> 00:00:04,633
那嵌入模型从简单到复杂

2
00:00:04,633 --> 00:00:07,081
它也是经历了一个发展和演变的过程

3
00:00:07,100 --> 00:00:08,633
从早期的Word2Vec

4
00:00:08,633 --> 00:00:10,823
这种基于词向量的模型

5
00:00:10,830 --> 00:00:12,790
到后边Transformer和BERT的出现

6
00:00:12,790 --> 00:00:14,211
是一个重大的转变

7
00:00:14,211 --> 00:00:17,109
然后再到现在的大语言模型基础上的嵌入

8
00:00:17,120 --> 00:00:18,730
那从早期是怎么一回事

9
00:00:18,735 --> 00:00:20,575
早期实际上Word2Vec

10
00:00:20,575 --> 00:00:23,105
把这个词嵌入成一个向量

11
00:00:23,110 --> 00:00:26,080
其实在当时是一个非常突破性的思想

12
00:00:26,090 --> 00:00:28,743
它是Google提出来的词嵌入模型

13
00:00:28,750 --> 00:00:32,839
包括连续词袋CBOW和跳字模型Skip-gram

14
00:00:32,845 --> 00:00:37,421
这些词就是根据它本身和它相邻的一些词中的规律

15
00:00:37,435 --> 00:00:41,335
从这些规律中学习逐渐捕捉到了一些语义信息

16
00:00:41,335 --> 00:00:47,185
那把这个词语转换成了低维向量的表示

17
00:00:47,200 --> 00:00:49,288
它这个机制虽然简单

18
00:00:49,288 --> 00:00:54,975
但是它这个思想就是捕捉词语之间跟它相邻的几个上下文

19
00:00:54,975 --> 00:00:58,644
我们从N-gram出发就是Gram就是前后几个词

20
00:00:58,644 --> 00:01:02,656
这种上下文的学习过程通过机器学习方式

21
00:01:02,656 --> 00:01:05,686
然后就把这个词转换成了向量表示

22
00:01:05,700 --> 00:01:06,966
实际上Word2Vec

23
00:01:06,966 --> 00:01:10,131
就把我们人类带入了这个词向量的

24
00:01:10,150 --> 00:01:11,987
这个比较奇妙的一个世界

25
00:01:11,995 --> 00:01:16,699
那么在它出现之后就一系列其他的词向量也好

26
00:01:16,710 --> 00:01:19,510
词嵌入的方法就都应运而生了

27
00:01:19,510 --> 00:01:22,855
你比如说早期另外一个非常有名的叫做GloVe

28
00:01:22,870 --> 00:01:26,782
这个斯坦福大学提出的另外一种词嵌入的方法

29
00:01:26,785 --> 00:01:32,225
它是通过构建这个词语的共现概率来训练模型

30
00:01:32,230 --> 00:01:34,534
它能够捕捉一些稀疏的数据

31
00:01:34,534 --> 00:01:38,254
还有一些全局方面的语义信息都捕捉得比较好

32
00:01:38,255 --> 00:01:40,735
然后FastText也是很有名的

33
00:01:40,735 --> 00:01:43,532
Facebook AI提出来的词嵌入模型

34
00:01:43,535 --> 00:01:47,430
那这些模型都是在一批语料中训练好了之后

35
00:01:47,445 --> 00:01:48,965
然后上传到网上

36
00:01:48,965 --> 00:01:50,865
然后就说这是我发布的

37
00:01:50,865 --> 00:01:53,097
我这一套词嵌入模型

38
00:01:53,097 --> 00:01:56,211
你们做NLP任务的研究学者可以下载

39
00:01:56,215 --> 00:01:59,185
那在当时我们做这些NLP项目的时候

40
00:01:59,190 --> 00:02:02,036
都是基于人家的这个GloVe和FastText

41
00:02:02,036 --> 00:02:04,133
这些训练好的词向量

42
00:02:04,135 --> 00:02:06,259
然后再去做我们的一些任务

43
00:02:06,260 --> 00:02:08,458
那当时这些模型都是非常有名的

44
00:02:08,460 --> 00:02:10,774
它也是在当时的任务解决方面

45
00:02:10,774 --> 00:02:12,376
也都是非常有效果的

46
00:02:12,385 --> 00:02:15,541
那么这就是Word Embedding Models

47
00:02:15,541 --> 00:02:17,478
的一个演化过程

48
00:02:17,480 --> 00:02:21,135
那么早期就是刚才所说的这一系列模型

49
00:02:21,135 --> 00:02:23,781
Word2Vec都是有名的

50
00:02:23,785 --> 00:02:28,058
这个就是它我们算是这个上下文无关的

51
00:02:28,060 --> 00:02:30,933
什么意思呢就是它这一批模型训练好了

52
00:02:30,933 --> 00:02:32,544
我这给你发布出来了

53
00:02:32,560 --> 00:02:33,848
包括这个Word2Vec

54
00:02:33,850 --> 00:02:36,122
包括GloVe 包括FastText

55
00:02:36,125 --> 00:02:40,880
但是这一批模型它没有办法灵活的

56
00:02:40,880 --> 00:02:46,050
根据一批新的语料再去重新进行新的调整

57
00:02:46,050 --> 00:02:48,995
没有办法它其实是一个相当于是一个固定的

58
00:02:48,995 --> 00:02:52,370
一个已经习得好了的这个词向量表

59
00:02:52,370 --> 00:02:55,635
那么每个词它的向量都被固定住了

60
00:02:55,635 --> 00:02:57,988
人家发布给你你就这样去用吧

61
00:02:57,990 --> 00:03:00,860
不再能够根据你的新语料

62
00:03:00,860 --> 00:03:04,448
去灵活地根据上下文自适应

63
00:03:04,450 --> 00:03:08,302
那这个是早期的上下文无关的词嵌入

64
00:03:08,310 --> 00:03:10,240
当然作为他们训练本身

65
00:03:10,240 --> 00:03:14,622
他们也是吸收了训练时的上下文语境

66
00:03:14,622 --> 00:03:15,868
那他是吸收了的

67
00:03:15,870 --> 00:03:19,423
只是他并不能够灵活地适应新的上下文

68
00:03:19,430 --> 00:03:22,598
那包括更早期的Bag of Words

69
00:03:22,600 --> 00:03:25,625
TF-IDF都是属于早期模型

70
00:03:25,630 --> 00:03:28,373
那么在右边出现的新技术

71
00:03:28,373 --> 00:03:29,850
就是上下文相关

72
00:03:29,865 --> 00:03:33,755
那么典型代表就是这个Transformer机制出现了

73
00:03:33,765 --> 00:03:36,910
那么就出现了一批能够以自注意力机制

74
00:03:36,915 --> 00:03:40,215
然后去灵活地训练大批语料

75
00:03:40,225 --> 00:03:44,563
然后根据你这个语料中间这个语义的关系

76
00:03:44,563 --> 00:03:48,330
来调整他的这个词嵌入的这个上下文

77
00:03:48,330 --> 00:03:50,696
然后他们就变得就非常的灵活

78
00:03:50,700 --> 00:03:54,678
你可以把你自己的这些语料输入到这些语料

79
00:03:54,685 --> 00:03:58,596
这些Bot和XLM这些Robot这些大模型里面去

80
00:03:58,596 --> 00:04:01,890
生成你的这个上下文相关的嵌入向量

81
00:04:01,895 --> 00:04:05,240
那么在那个阶段我们的词向量

82
00:04:05,240 --> 00:04:08,139
就全部都变成了这一批所谓的

83
00:04:08,139 --> 00:04:11,362
基于Transformer架构的模型的这种训练方式

84
00:04:11,365 --> 00:04:13,165
当然在Transformer出现之前

85
00:04:13,165 --> 00:04:16,895
我们也有基于RNN的词嵌入模型也是有的

86
00:04:16,905 --> 00:04:19,677
所以说这个历史发展过程是逐渐

87
00:04:19,677 --> 00:04:23,890
从上下文无关到上下文相关的动态的学习型的

88
00:04:25,710 --> 00:04:29,838
那么当词嵌入已经逐渐的不能够满足

89
00:04:29,838 --> 00:04:34,284
我们对语义细节的要求对长上下文的要求

90
00:04:34,284 --> 00:04:37,254
那么逐渐的就出现了句子嵌入模型

91
00:04:37,254 --> 00:04:39,540
和Sentence Transformer这种框架

92
00:04:39,545 --> 00:04:42,509
那直到现在Sentence Transformer还是

93
00:04:42,509 --> 00:04:46,463
基本上还是我们现代嵌入模型的一个最基本的架构

94
00:04:46,475 --> 00:04:49,931
它就是把整句整段话或者说整个文本

95
00:04:49,931 --> 00:04:51,875
进行一个整体的理解

96
00:04:51,875 --> 00:04:54,288
通过Transformer和自注意力机制

97
00:04:54,288 --> 00:04:56,733
来形成好的嵌入模型

98
00:04:56,735 --> 00:05:00,641
那么这后面的很多细节呢实际上是

99
00:05:00,641 --> 00:05:03,998
需要你如果有兴趣可能你可以去阅读

100
00:05:03,998 --> 00:05:07,415
比如说这个GPT图解大模型是怎样构建的

101
00:05:07,420 --> 00:05:09,591
这个我写的一本书就是专门讲

102
00:05:09,591 --> 00:05:13,580
我们是怎样用这个Transformer这种模型做词嵌入的

103
00:05:13,580 --> 00:05:16,660
我把这个从古到今也不是很古了

104
00:05:16,660 --> 00:05:21,502
大概是20年之前这些Word2Vec到这个RNN再到

105
00:05:21,510 --> 00:05:24,450
Transformer它是怎么一系列的进化

106
00:05:24,455 --> 00:05:26,345
其实这个细节讲的还是比较清楚

107
00:05:26,345 --> 00:05:27,695
但是作为我们应用者呢

108
00:05:27,710 --> 00:05:29,503
我们就只是使用就可以了

109
00:05:29,505 --> 00:05:32,865
那么我们就去用这个Transformer

110
00:05:32,865 --> 00:05:36,580
这个框架里面给我们训练好的模型

111
00:05:36,580 --> 00:05:38,946
拿过来用它也都是开源的模型

112
00:05:38,950 --> 00:05:40,960
就已经能够基本上完成

113
00:05:40,960 --> 00:05:44,377
这个相似度比较和很多常规的NLP任务了

114
00:05:44,395 --> 00:05:46,696
那我给你列出了是几个常见的

115
00:05:46,696 --> 00:05:48,423
Sentence Transformer的模型

116
00:05:48,425 --> 00:05:51,142
那我们你可以去网上去看

117
00:05:51,142 --> 00:05:52,790
这个Sentence Transformer

118
00:05:52,790 --> 00:05:54,668
它这个库都有哪些还有它的运作

119
00:05:57,390 --> 00:05:59,953
它有一个专门的一个网站

120
00:05:59,953 --> 00:06:02,982
里面就有讲它所有的这个模型

121
00:06:02,982 --> 00:06:04,922
都是怎么训练出来的

122
00:06:04,922 --> 00:06:07,178
然后它这个框架是怎么使用

123
00:06:07,185 --> 00:06:08,977
这叫做SBERT

124
00:06:08,977 --> 00:06:12,438
就是基于BERT和Transformer的架构

125
00:06:12,438 --> 00:06:16,456
然后去训练一系列的这个Encoder

126
00:06:16,460 --> 00:06:21,180
然后又跟Hugging Face集成起来了

127
00:06:21,180 --> 00:06:24,997
它是UKPLab创建出来的

128
00:06:25,000 --> 00:06:28,249
然后现在是由Hugging Face做维护

129
00:06:28,255 --> 00:06:31,286
所以所有的Hugging Face里面的嵌入模型

130
00:06:31,286 --> 00:06:33,164
几乎都给你提供一个

131
00:06:33,164 --> 00:06:35,570
Sentence Transformer的一个实现版

132
00:06:35,570 --> 00:06:37,990
那你就可以非常轻松地

133
00:06:37,990 --> 00:06:40,190
把它应用到你的项目中

134
00:06:40,205 --> 00:06:42,361
我在我的这一系列的示例项目中

135
00:06:42,361 --> 00:06:44,055
已经给大家提供了大量的

136
00:06:44,055 --> 00:06:46,360
基于Sentence Transformer的嵌入模型

137
00:06:46,360 --> 00:06:48,300
你可以去读读它的文档

138
00:06:48,300 --> 00:06:50,628
然后要使用起来是非常容易的

139
00:06:50,635 --> 00:06:53,320
随便去Hugging Face选择一个

140
00:06:53,320 --> 00:06:54,670
选择一个基于Sentence Transformer的嵌入模型

141
00:06:56,430 --> 00:06:58,714
Hugging Face的Sentence Transformer

142
00:06:58,714 --> 00:07:00,086
它都会给你提供

143
00:07:00,086 --> 00:07:02,524
如何使用它的示例代码的实现

144
00:07:02,525 --> 00:07:05,177
你比如说你就可以安装一下

145
00:07:05,177 --> 00:07:08,237
当然我们的这个标准库里面都已经

146
00:07:08,250 --> 00:07:10,200
把这个Sentence Transformer都安装好了

147
00:07:10,200 --> 00:07:14,997
比如说一个模型你只要导入它

148
00:07:14,997 --> 00:07:19,056
然后指定它下载下来就好

149
00:07:19,065 --> 00:07:24,933
我看如果说我们搜索BGE这个模型

150
00:07:24,935 --> 00:07:27,894
看看它是不是也有这个Sentence Transformer的版本

151
00:07:27,894 --> 00:07:31,214
如何使用它呢大家看果然有

152
00:07:31,214 --> 00:07:34,842
就是它其实也是基于Sentence Transformer的模型

153
00:07:34,845 --> 00:07:37,741
那么你在Hugging Face上下载下来这个BGE

154
00:07:37,750 --> 00:07:42,048
我们的这个M3多语言多类型吧

155
00:07:42,060 --> 00:07:47,064
就是既有稀疏也有这个密集这种检索方式

156
00:07:47,075 --> 00:07:48,945
它这个有三个M有三个多

157
00:07:48,945 --> 00:07:51,155
哪三个多回来我们可以再讲解

158
00:07:51,175 --> 00:07:54,520
那么它这个library里面就说你想使用它

159
00:07:54,530 --> 00:07:57,379
你可以就是看看它是如何在这个Sentence

160
00:07:57,390 --> 00:07:59,150
Transformer中被导入

161
00:07:59,150 --> 00:08:02,990
然后去使用这个BGE M3这个模型的

162
00:08:03,000 --> 00:08:07,427
这个意思就是说很多嵌入模型都是基于Sentence Transformer这个架构

163
00:08:07,430 --> 00:08:09,262
那么你就去Hugging Face

164
00:08:09,265 --> 00:08:13,690
它就会给你这个基于Sentence Transformer的这个示例代码实现

165
00:08:13,690 --> 00:08:16,380
你可以基于这个去比较它的similarity

166
00:08:16,380 --> 00:08:19,780
这个就是嵌入模型的最基本的完整的使用方式

167
00:08:19,795 --> 00:08:20,789
不外乎就是如此

168
00:08:21,810 --> 00:08:24,514
那我们回到这来我们现在就发展到了

169
00:08:24,525 --> 00:08:27,874
现在大语言模型的嵌入就是基于这个Sentence

170
00:08:27,890 --> 00:08:28,695
Transformer的架构

171
00:08:28,695 --> 00:08:31,863
那么再进一步发展我们看后续还有什么呢

172
00:08:31,875 --> 00:08:36,135
后续又出现了一些比如说多语言的嵌入模型

173
00:08:36,145 --> 00:08:37,684
这有几种都是早期的

174
00:08:37,684 --> 00:08:40,059
那实际上现在所有的包括OpenAI

175
00:08:40,059 --> 00:08:42,531
包括刚才我们看到的BGE M3

176
00:08:42,531 --> 00:08:44,466
都是多语言的嵌入模型

177
00:08:44,466 --> 00:08:47,727
那么后来又有了这个多模态的嵌入模型对吧

178
00:08:47,727 --> 00:08:49,479
那么这个就是一个新的趋势

179
00:08:49,490 --> 00:08:52,890
因为多模态它就是要求我们在做embedding的时候

180
00:08:54,670 --> 00:08:55,635
全统一起来

181
00:08:55,635 --> 00:08:58,127
还要把这个文本图像视频音频

182
00:08:58,127 --> 00:09:00,759
全部都映射到同一个向量空间中

183
00:09:00,775 --> 00:09:03,127
这样的话我再搜索一只可爱的小狗狗

184
00:09:03,135 --> 00:09:07,139
我不仅仅能够搜索出来一批这个文字对狗狗的描述

185
00:09:07,140 --> 00:09:10,598
我还能搜到一批狗狗的图片甚至它叫的声音

186
00:09:10,600 --> 00:09:11,575
这些东西我

187
00:09:11,575 --> 00:09:15,874
们都给它全部embedding到同一个空间里面去做一些事情

188
00:09:15,874 --> 00:09:18,559
这个就是OpenAI也是OpenAI给我们的第一步

189
00:09:18,560 --> 00:09:21,877
这种是它叫当时叫做这个CLIP这个模型

190
00:09:21,877 --> 00:09:24,660
CLIP就做了这个多模态模型

191
00:09:24,665 --> 00:09:28,730
就是OpenAI的整合文本和图像它叫做对齐

192
00:09:28,730 --> 00:09:31,337
我们把它不同的两种东西

193
00:09:31,345 --> 00:09:34,592
不同的模态整合到同一个空间中去对齐

194
00:09:34,595 --> 00:09:37,619
你就能够把小狗狗和狗狗的图片映射得上

195
00:09:37,625 --> 00:09:40,625
我想我们的大脑也一定是做了这样的一个对齐

196
00:09:40,625 --> 00:09:42,921
就是说我们看到狗狗这个字脑海

197
00:09:42,921 --> 00:09:44,889
中会浮现出来狗狗的画面

198
00:09:44,900 --> 00:09:48,320
那么大模型时代我们多模态也是在做一样的事

199
00:09:48,330 --> 00:09:51,732
就是捕捉各种各样非结构化的数据与信息

200
00:09:51,745 --> 00:09:54,655
然后告诉大模型它们是一个东西是一个东西

201
00:09:54,655 --> 00:09:58,798
那么逐渐的它就能够相互的做这个正向或者反向的搜索

202
00:09:58,798 --> 00:10:02,915
看到狗狗想到图像，看到狗狗的图片想到狗狗的文字

203
00:10:02,915 --> 00:10:07,170
这样的话将来的这个大模型系统它可就不仅仅是语言

204
00:10:07,170 --> 00:10:09,815
它还能够融合多模态的信息细节

205
00:10:09,815 --> 00:10:12,275
我们可以去看这个CLIP的论文

206
00:10:12,280 --> 00:10:14,249
但是我们从宏观上面了解

207
00:10:14,249 --> 00:10:18,834
我们了解到这个多模态的嵌入空间的对齐也就可以了

208
00:10:18,835 --> 00:10:21,845
好，那就是这个最基本的

209
00:10:21,845 --> 00:10:24,640
大语言模型到目前时代的一个

210
00:10:24,650 --> 00:10:25,601
模型这个发展

211
00:10:25,601 --> 00:10:30,983
下面我们就去讲现在我们通用的大圆模型都是有哪些

212
00:10:31,005 --> 00:10:34,329
进入到大圆模型时代的嵌入


大模型时代的嵌入技术：
1
00:00:00,000 --> 00:00:02,222
来讲大元时代的嵌入模型

2
00:00:02,222 --> 00:00:05,050
也就是我们现在日常工作中做RAG

3
00:00:05,050 --> 00:00:09,095
所需要用到的这些现代嵌入模型

4
00:00:09,105 --> 00:00:13,065
那大元时代的嵌入模型选择太多了

5
00:00:13,070 --> 00:00:17,502
有点选择困难就是我们其实不太知道

6
00:00:17,502 --> 00:00:20,673
说到底应该用哪个那怎么办

7
00:00:20,675 --> 00:00:23,825
这里有一个MTEB排行榜

8
00:00:23,825 --> 00:00:28,725
可以帮助我们做一个基本的过滤

9
00:00:28,740 --> 00:00:31,249
那么这个排行榜它有四个维度

10
00:00:31,249 --> 00:00:33,824
我给大家说一下这个表怎么看

11
00:00:33,825 --> 00:00:38,063
它第一个维度当然是在横轴

12
00:00:38,063 --> 00:00:43,066
这个是模型的大小这个维度当然很重要

13
00:00:43,066 --> 00:00:45,178
就是你有多大的GPU

14
00:00:45,185 --> 00:00:47,275
你选择多大的嵌入模型

15
00:00:47,275 --> 00:00:50,135
那么嵌入模型一般来说它都是

16
00:00:50,135 --> 00:00:52,115
比生成模型要小一些

17
00:00:52,130 --> 00:00:55,082
最大的现在它有一个100B的

18
00:00:55,085 --> 00:00:56,354
但是这个只是这一个

19
00:00:56,360 --> 00:00:59,490
大部分还都在10B以下

20
00:00:59,490 --> 00:01:02,620
也就是说一个中等的GPU

21
00:01:02,620 --> 00:01:06,160
比如说4090 32GB内存双卡

22
00:01:06,160 --> 00:01:09,112
基本上是可以完成绝大多数

23
00:01:09,112 --> 00:01:12,310
的主流嵌入模型的嵌入工作的

24
00:01:12,330 --> 00:01:15,322
是开源的现在我们说的MTEB

25
00:01:15,322 --> 00:01:18,454
都是指的是开源的嵌入模型

26
00:01:18,465 --> 00:01:20,819
这是它的大小维度很重要

27
00:01:20,825 --> 00:01:23,693
第二个维度是它的能力维度

28
00:01:23,693 --> 00:01:27,393
这个是能力这个能力是怎么计算出来的

29
00:01:27,395 --> 00:01:29,505
它是基于一系列的任务

30
00:01:29,505 --> 00:01:32,081
取这个平均值min就是平均分

31
00:01:32,081 --> 00:01:34,673
它在各种各样的数据集上面

32
00:01:34,680 --> 00:01:37,696
比如说有做分类的有做聚类的

33
00:01:37,696 --> 00:01:40,890
有做检索的一系列的NLP任务

34
00:01:40,890 --> 00:01:43,500
都拿这些模型做嵌入

35
00:01:43,505 --> 00:01:48,653
然后看看哪个模型如果其他的算法都一样

36
00:01:48,660 --> 00:01:51,063
就只是嵌入模型更换

37
00:01:51,063 --> 00:01:53,733
看看它们这些嵌入模型

38
00:01:53,733 --> 00:01:56,805
哪些个模型任务完成的更好

39
00:01:56,805 --> 00:01:58,725
它的平均分就越高

40
00:01:58,735 --> 00:02:02,101
那当然它有些模型是专注于某一类任务的

41
00:02:02,110 --> 00:02:04,686
那么这些模型可能就在某类任务突出

42
00:02:04,690 --> 00:02:07,874
所以说我们看这个其实它是一个总榜

43
00:02:07,875 --> 00:02:10,359
是个平均分这是第二个维度

44
00:02:10,360 --> 00:02:12,520
那么为什么我说其实它有四个维度呢

45
00:02:12,525 --> 00:02:15,385
大家看这儿还有一个奥秘

46
00:02:15,385 --> 00:02:20,074
它这个圈的大小代表了嵌入的维度的大小

47
00:02:20,074 --> 00:02:23,122
那我们说一个二维的坐标系

48
00:02:23,125 --> 00:02:24,916
你只能展现两个维度

49
00:02:24,916 --> 00:02:29,085
那如果我想展现第三个维度的信息怎么展现

50
00:02:29,085 --> 00:02:32,940
你可以通过这个圆圈我们也叫气泡

51
00:02:32,950 --> 00:02:36,086
这种气泡图的泡泡的大小来看

52
00:02:36,086 --> 00:02:39,265
如果说这个泡特别大你就知道OK

53
00:02:39,265 --> 00:02:42,675
这个模型它的embedding的维度比较大

54
00:02:42,675 --> 00:02:44,820
也就是说同样大小的句子

55
00:02:44,820 --> 00:02:48,142
都是这么长的句子你的嵌入维度越大

56
00:02:48,142 --> 00:02:50,989
你可能能够表示的细节就越多

57
00:02:50,995 --> 00:02:53,025
每一个维度它都是捕捉

58
00:02:53,025 --> 00:02:56,273
其实一个程度上的某个维度的特征嘛

59
00:02:56,295 --> 00:02:58,253
你的维度越多你就能够说

60
00:02:58,253 --> 00:03:00,567
越多的比较细微的特征的差异

61
00:03:00,580 --> 00:03:03,268
这个也是很重要的一个考量

62
00:03:03,270 --> 00:03:07,736
那当然通常来说模型的参数越大

63
00:03:07,745 --> 00:03:12,425
那么它的可支持的嵌入维度也就越大

64
00:03:12,425 --> 00:03:15,743
但是它不是一个线性的关系对吧

65
00:03:15,745 --> 00:03:18,055
那有的模型只有一个B链

66
00:03:18,055 --> 00:03:21,692
你比如说这几个模型它虽然它的参数不多

67
00:03:21,692 --> 00:03:24,710
但是它呢这个embedding的维度特别大

68
00:03:24,710 --> 00:03:28,434
也有这种情况一般来说你看这一批小模型家族

69
00:03:28,445 --> 00:03:32,305
它的这个embedding size嵌入维度也是偏小的

70
00:03:32,310 --> 00:03:33,814
而这个大模型家族

71
00:03:33,814 --> 00:03:36,506
它的embedding size也都是偏大的

72
00:03:36,506 --> 00:03:39,495
但不是一概而论你看中部有几个模型

73
00:03:39,495 --> 00:03:42,695
就是说参数大小一般然后嵌入维度特别大

74
00:03:42,695 --> 00:03:44,358
也是有这种情况的OK

75
00:03:44,358 --> 00:03:47,480
那然后我还说第四个维度是在哪

76
00:03:47,480 --> 00:03:48,656
第四个维度在这儿

77
00:03:49,700 --> 00:03:51,284
这个叫做颜色维度

78
00:03:51,284 --> 00:03:53,462
就是它通过颜色来区分的

79
00:03:53,470 --> 00:03:55,530
就是因为这个横轴用了

80
00:03:55,530 --> 00:03:58,425
纵轴用了这个气泡大小也用了

81
00:03:58,430 --> 00:04:00,691
那怎么办我还想再去区分

82
00:04:00,691 --> 00:04:03,661
这个某一个维度那就只能上颜色了

83
00:04:03,680 --> 00:04:06,713
这个也很重要这个叫做max tokens

84
00:04:06,713 --> 00:04:09,841
这是上下文我这个嵌入模型

85
00:04:09,841 --> 00:04:13,453
最多能够接纳多长的嵌入上下文

86
00:04:13,455 --> 00:04:15,444
你这个句子或者段落

87
00:04:15,444 --> 00:04:17,654
最多能够有多少个token

88
00:04:17,655 --> 00:04:19,527
那这是相当重要的一个维度

89
00:04:19,535 --> 00:04:22,055
当然如果它在训练也好

90
00:04:22,055 --> 00:04:24,323
在这个使用时候也好

91
00:04:24,335 --> 00:04:26,740
能够接纳的上下文越长

92
00:04:26,740 --> 00:04:29,320
说明它这个模型的潜力能力

93
00:04:29,320 --> 00:04:32,136
它也就相对来说能够满足

94
00:04:32,136 --> 00:04:33,928
我们越大的需求

95
00:04:33,935 --> 00:04:35,375
因为现代有的时候

96
00:04:35,375 --> 00:04:38,075
我们确实是有这个长上下文的需求

97
00:04:38,090 --> 00:04:39,692
我需要把很多的东西

98
00:04:39,692 --> 00:04:41,472
都给它输入到模型里面

99
00:04:41,475 --> 00:04:42,770
让它整体做嵌入

100
00:04:42,775 --> 00:04:44,375
但是我们不要忘了

101
00:04:44,375 --> 00:04:47,098
这个是嵌入模型这不是生成模型

102
00:04:47,100 --> 00:04:49,300
它的上下文不是越长越好

103
00:04:49,300 --> 00:04:51,100
而是说你还是要把它

104
00:04:51,100 --> 00:04:53,388
根据语义切分成一个一个

105
00:04:53,388 --> 00:04:57,160
比较能够有这个内容的段落或者句子

106
00:04:57,160 --> 00:04:59,090
所以通常绝大多数来讲

107
00:04:59,090 --> 00:05:01,600
你还是要把它一千个token以内

108
00:05:01,600 --> 00:05:04,078
做嵌入这样的话比较合适

109
00:05:04,078 --> 00:05:05,886
比较利于模型把握

110
00:05:05,886 --> 00:05:08,964
相对来说这个细微的语义和内容

111
00:05:08,965 --> 00:05:11,293
对于检索的工作

112
00:05:11,293 --> 00:05:14,494
也相对来说比较合适合理

113
00:05:14,505 --> 00:05:17,065
这就是四个维度

114
00:05:17,065 --> 00:05:19,626
第一是大小第二是模型的分数

115
00:05:19,626 --> 00:05:22,496
第三是模型的embedding维度

116
00:05:22,496 --> 00:05:26,305
第四个是模型可以接纳的token数

117
00:05:26,315 --> 00:05:28,079
也就是它能够接纳的

118
00:05:28,471 --> 00:05:30,627
所做嵌入的上下文长度

119
00:05:30,635 --> 00:05:34,768
好那我们要具体来看一看

120
00:05:34,768 --> 00:05:36,595
这个MTEB是给我们

121
00:05:36,595 --> 00:05:38,985
划分成了哪八大类任务

122
00:05:38,985 --> 00:05:41,031
里边有各种各样的数据集

123
00:05:41,035 --> 00:05:43,480
那这些数据集都是很学术的东西了

124
00:05:43,490 --> 00:05:45,806
我们就不需要去一个数据集

125
00:05:45,806 --> 00:05:47,543
一个数据集的去了解

126
00:05:47,560 --> 00:05:49,960
因为那个真的是我们要做NLP

127
00:05:49,960 --> 00:05:52,160
这个学科领域的博士研究

128
00:05:57,680 --> 00:05:59,792
然后去看看他们的哪个模型

129
00:05:59,792 --> 00:06:01,728
然后处理哪一方面的任务

130
00:06:01,735 --> 00:06:03,705
然后设计什么样的算法

131
00:06:03,705 --> 00:06:05,084
让他做得更精准

132
00:06:05,095 --> 00:06:07,655
你比如说我就曾经常用

133
00:06:07,655 --> 00:06:09,703
这个原来的IMDB

134
00:06:09,703 --> 00:06:12,825
这个数据集因为这个数据集

135
00:06:12,825 --> 00:06:15,355
一直是用这个电影来评论

136
00:06:15,365 --> 00:06:17,345
就是评论这个影评的好坏

137
00:06:17,350 --> 00:06:19,726
这些都是蛮常用的数据集

138
00:06:19,730 --> 00:06:21,911
然后这个亚马逊的review

139
00:06:21,911 --> 00:06:23,873
因为原本NLP分类任务

140
00:06:23,873 --> 00:06:26,264
是个非常基本而常用的任务

141
00:06:26,270 --> 00:06:28,404
其实不过呢现在这些任务

142
00:06:28,404 --> 00:06:30,038
都相对合流了其实

143
00:06:30,038 --> 00:06:32,612
就逐渐的被这个大圆模型

144
00:06:32,612 --> 00:06:34,111
一统江湖就是说

145
00:06:34,111 --> 00:06:36,542
所有的这些任务能力强的模型

146
00:06:36,542 --> 00:06:37,664
应该来说都强

147
00:06:37,685 --> 00:06:39,753
能力弱的模型可能他都弱

148
00:06:39,753 --> 00:06:42,197
现在可能存在于一个这种现象

149
00:06:42,220 --> 00:06:43,840
那我们用中文来看看

150
00:06:43,840 --> 00:06:46,180
这八大任务到底都是什么任务

151
00:06:46,195 --> 00:06:50,264
这八大任务第一个就是聚类

152
00:06:50,275 --> 00:06:51,675
聚类是什么意思呢

153
00:06:51,675 --> 00:06:53,765
就是相似的文本聚在一起

154
00:06:53,765 --> 00:06:55,475
它是一个无监督学习

155
00:06:55,480 --> 00:06:57,449
就是一大堆图片搁在一起

156
00:06:57,449 --> 00:06:59,619
你告诉我随便把它分成几类

157
00:06:59,619 --> 00:07:01,570
然后这个动物一类鸟类一类

158
00:07:01,570 --> 00:07:03,440
但其实聚类是没有标签的

159
00:07:03,450 --> 00:07:05,205
它给你聚完类之后呢

160
00:07:05,205 --> 00:07:07,350
它并不知道哪一堆是人物

161
00:07:07,365 --> 00:07:09,201
哪一堆是动物哪一堆是鸟类

162
00:07:09,201 --> 00:07:11,233
它不知道它就是把这些东西

163
00:07:11,233 --> 00:07:12,465
给你分门别类的

164
00:07:12,465 --> 00:07:14,225
给你排成一个一个的簇

165
00:07:14,235 --> 00:07:16,688
这是聚类那分类就不同

166
00:07:16,688 --> 00:07:18,712
虽然也是分门别类

167
00:07:18,725 --> 00:07:21,780
但是它是在做这个任务的时候

168
00:07:21,780 --> 00:07:24,835
已经把这些文本都打上标签了

169
00:07:25,960 --> 00:07:27,995
然后你遇到新的文本之后

170
00:07:27,995 --> 00:07:31,140
你再按照这个固定的这些好评中评差评

171
00:07:31,140 --> 00:07:32,132
给它去归类

172
00:07:32,132 --> 00:07:34,508
所以这就是一个有监督学习

173
00:07:34,510 --> 00:07:36,130
这是分类任务

174
00:07:36,130 --> 00:07:38,450
然后这个双语挖掘任务

175
00:07:38,450 --> 00:07:41,374
两个语言中的句子对给它对齐

176
00:07:41,374 --> 00:07:43,022
一个英文一个中文

177
00:07:43,022 --> 00:07:46,248
要你把它找出来哪个是匹配得上的

178
00:07:46,248 --> 00:07:48,016
在两个语言中统一

179
00:07:48,025 --> 00:07:49,978
然后还有这个评估STS

180
00:07:49,978 --> 00:07:52,799
评估两个句子的语义有多相近

181
00:07:52,820 --> 00:07:55,452
这是一个回归任务是打分的任务

182
00:07:55,460 --> 00:07:57,575
这个任务就相对来说

183
00:07:57,575 --> 00:07:58,985
又不是说分类

184
00:07:58,990 --> 00:08:01,681
它可能是最后会给出一个分值

185
00:08:01,681 --> 00:08:04,991
它这个数据集里面已经有一些分值了

186
00:08:04,991 --> 00:08:07,665
然后你通过它这些数据集去训练

187
00:08:07,670 --> 00:08:09,407
然后遇到类似的任务

188
00:08:09,407 --> 00:08:10,951
你也给出一个分值

189
00:08:10,955 --> 00:08:12,791
然后这个对分类判断

190
00:08:12,791 --> 00:08:16,615
两个文本是不是有某种特定关系比如说FAQ

191
00:08:16,615 --> 00:08:19,975
这两个文本并不一定是说很相似

192
00:08:19,985 --> 00:08:21,452
你比如说问题和答案

193
00:08:21,452 --> 00:08:22,593
你问出一个问题

194
00:08:25,480 --> 00:08:27,442
你能不能够判断出来

195
00:08:27,442 --> 00:08:31,148
相应的这个答案是不是针对这个问题的

196
00:08:31,150 --> 00:08:32,820
其实放在大模型时代

197
00:08:32,820 --> 00:08:34,824
这些任务全部都迎刃而解了

198
00:08:34,830 --> 00:08:36,639
都不再是难题了已经

199
00:08:36,640 --> 00:08:37,858
然后检索检索

200
00:08:37,858 --> 00:08:39,888
这就是我们所做的工作

201
00:08:39,900 --> 00:08:42,199
就是问答系统RAG文档检索

202
00:08:42,205 --> 00:08:45,370
相对来说这是一个比较统一的任务

203
00:08:45,375 --> 00:08:46,950
其实检索它包含

204
00:08:46,950 --> 00:08:48,830
刚才我们说的句类分类

205
00:08:48,830 --> 00:08:52,355
对分类判断两个句子语义有多相近

206
00:08:52,360 --> 00:08:55,374
都可以统一成检索任务

207
00:08:55,375 --> 00:08:56,491
在这个范畴下

208
00:08:56,491 --> 00:08:59,095
然后摘要看看这个嵌入模型

209
00:08:59,095 --> 00:09:00,211
做了嵌入之后

210
00:09:00,225 --> 00:09:03,285
然后它这个下游做摘要任务的时候

211
00:09:03,285 --> 00:09:04,305
能力有多强

212
00:09:04,320 --> 00:09:06,634
然后这个重排序ranking

213
00:09:06,634 --> 00:09:08,176
也是一个任务

214
00:09:08,185 --> 00:09:11,685
给出一二三四五六七个候选答案

215
00:09:11,685 --> 00:09:14,544
你呢在这个一堆候选答案里面

216
00:09:14,544 --> 00:09:17,968
你找到跟这个问题最相关的那个答案

217
00:09:17,975 --> 00:09:19,495
这就是重排序ranking

218
00:09:19,495 --> 00:09:22,461
在咱们RAG里面这也是一个很重要的环节

219
00:09:22,461 --> 00:09:25,715
所以把这些任务所有的数据集都有相关数据

220
00:09:25,715 --> 00:09:28,520
综合起来给它做一个评判

221
00:09:28,520 --> 00:09:30,452
给这个模型打分

222
00:09:30,452 --> 00:09:34,316
这就是我们MTEB排行榜的重要性

223
00:09:34,320 --> 00:09:39,136
好那我们就去MTEB这个排行榜去看一看

224
00:09:39,136 --> 00:09:41,544
哪些模型都在榜上

225
00:09:41,550 --> 00:09:43,913
那其实领先的模型我也给它截了一个屏

226
00:09:43,925 --> 00:09:47,838
实际上我们看到现在目前来讲

227
00:09:47,838 --> 00:09:52,853
其实我们的这个签问系列的这个模型

228
00:09:52,853 --> 00:09:57,783
是蛮有名的也蛮领先的

229
00:09:57,783 --> 00:10:02,430
然后这个Germany它是一个通过API调用的模型

230
00:10:02,430 --> 00:10:04,250
它并不是属于一个完全开源的

231
00:10:04,255 --> 00:10:07,842
所以你看不到这个第一名的这个Germany Embedding

232
00:10:07,842 --> 00:10:10,015
Google的Germany Embedding模型

233
00:10:10,015 --> 00:10:14,588
它其实并没有说可以随便在Hugging Face上面下载下来

234
00:10:14,588 --> 00:10:17,804
好那么说完了MTEB这个排行榜

235
00:10:17,804 --> 00:10:20,050
和一系列的开源模型

236
00:10:20,050 --> 00:10:22,990
我们现在就来看一下闭源的模型

237
00:10:23,000 --> 00:10:24,859
商用的模型都有哪些可供用的

238
00:10:24,859 --> 00:10:26,003
有哪些可供选择的

239
00:10:26,015 --> 00:10:30,149
我们从这个OpenAI Embedding Models先来开始讲

240
00:10:30,149 --> 00:10:31,615
那其实OpenAI Embedding

241
00:10:31,615 --> 00:10:33,758
在我的这个做项目的过程中

242
00:10:33,758 --> 00:10:36,526
我感觉它其实比其他的一些商用模型

243
00:10:36,526 --> 00:10:38,060
还是有这个领先地位的

244
00:10:38,060 --> 00:10:39,260
因为其实大家可想而知

245
00:10:39,265 --> 00:10:41,602
OpenAI GPT家族的模型

246
00:10:41,605 --> 00:10:43,657
包括现在最新的4.5

247
00:10:43,657 --> 00:10:45,730
包括O1、O3、4O

248
00:10:45,730 --> 00:10:51,240
为什么一直能够占据着这个比较领先的地位

249
00:10:51,255 --> 00:10:53,973
其实它一定是它的Embedding Model

250
00:10:53,973 --> 00:10:56,225
也有它的这个优势是在这里

251
00:10:56,225 --> 00:10:58,915
它的语料的收集也好训练的方式也好

252
00:10:58,915 --> 00:11:00,310
一定有它的过人之处

253
00:11:00,310 --> 00:11:02,413
所以虽然它的Embedding Models

254
00:11:02,413 --> 00:11:06,868
并不像它的GPT家族的生成模型那么有名

255
00:11:06,880 --> 00:11:10,435
但是其实也是非常良好的一个选择

256
00:11:10,440 --> 00:11:13,916
如果当你觉得你的开源模型嵌入效果不给力的时候

257
00:11:13,930 --> 00:11:16,954
你不妨用OpenAI的Embedding模型测试一下

258
00:11:16,955 --> 00:11:20,321
看一看商用模型它能够达到一个什么效果

259
00:11:20,330 --> 00:11:22,858
这个应该其实就是我们所能够摸到的

260
00:11:22,860 --> 00:11:25,357
尤其是这个Text Embedding 3 Large

261
00:11:25,360 --> 00:11:30,325
我们能够摸到的一个天花板的效果就是在这儿

262
00:11:30,325 --> 00:11:34,229
所以看有同学问说这个OpenAI的Embedding

263
00:11:34,230 --> 00:11:37,170
如果你做一个像我们做项目

264
00:11:37,180 --> 00:11:41,072
做这个所有的100万行数据的SnowMed

265
00:11:41,072 --> 00:11:44,950
那个医学名词数据全部做了大概多少钱

266
00:11:44,950 --> 00:11:49,229
那实际上就是说它是1美金

267
00:11:49,229 --> 00:11:54,419
它能够做嵌入做到9615页

268
00:11:55,600 --> 00:12:01,110
所以说可能能够做个一两百万个条数据

269
00:12:01,115 --> 00:12:02,435
应该是没有什么问题的

270
00:12:02,435 --> 00:12:04,465
那么如果你用Small的话那就更便宜

271
00:12:04,465 --> 00:12:05,734
比较便宜的多得多了

272
00:12:05,745 --> 00:12:08,179
如果你用Text Embedding 3 Small

273
00:12:08,185 --> 00:12:11,479
就是学一点的这个嵌入模型就便宜好多倍

274
00:12:11,495 --> 00:12:14,315
所以说它不会说太贵实际上

275
00:12:15,860 --> 00:12:18,832
但我们通常要做的Embedding的文档都特别多

276
00:12:18,845 --> 00:12:22,460
也特别庞大所以呢这个加到一块儿

277
00:12:22,470 --> 00:12:23,919
还是一笔不小的成本

278
00:12:23,919 --> 00:12:25,368
因为它跟生成不一样

279
00:12:25,375 --> 00:12:29,255
它是你所有的知识库里面的所有的文档和数据

280
00:12:29,255 --> 00:12:30,613
都需要被做嵌入

281
00:12:30,635 --> 00:12:35,008
而生成只是说我只检索出来其中的这个Top 20 Top 10

282
00:12:35,008 --> 00:12:37,756
我给传输到生成模型里面去

283
00:12:37,765 --> 00:12:40,157
所以它的这个概念是不一样的

284
00:12:40,157 --> 00:12:41,997
数据的量级是不一样的

285
00:12:42,010 --> 00:12:45,828
那么此外我们还有BGE Embedding

286
00:12:45,835 --> 00:12:48,958
那OpenAI这个Embedding是多语言

287
00:12:48,965 --> 00:12:51,143
中文英文都能够胜任

288
00:12:51,143 --> 00:12:54,289
而咱们的这个就是BAAI这个组织

289
00:12:54,305 --> 00:12:57,266
也就是智源做的这一套FlagOpen

290
00:12:57,266 --> 00:13:01,325
大模型开源技术体系也是支持多语言

291
00:13:01,325 --> 00:13:04,125
它有专门做的中文模型

292
00:13:04,135 --> 00:13:06,375
也有专门做的英文模型

293
00:13:06,375 --> 00:13:09,399
也有这个混合模型就是多语言支持的

294
00:13:09,399 --> 00:13:12,357
它都有所以这一套BGE家族的模型

295
00:13:12,357 --> 00:13:13,946
覆盖的非常全面

296
00:13:13,960 --> 00:13:16,100
那在这个MTEB排行榜上呢

297
00:13:16,100 --> 00:13:19,521
也往往都是处在一个比较靠前的位置

298
00:13:19,521 --> 00:13:23,145
所以大家可以去看一看这个FlagEmbedding网站

299
00:13:23,145 --> 00:13:25,400
看这个Leaderboard过来了没有

300
00:13:27,080 --> 00:13:29,925
那么这个Leaderboard里面像现在的Q1

301
00:13:29,925 --> 00:13:33,463
也是排在前边的Multilingual-E5-large

302
00:13:33,463 --> 00:13:38,417
也是在前边然后还有这个ST5

303
00:13:38,425 --> 00:13:40,752
这个比较小的模型1.5B连的

304
00:13:40,755 --> 00:13:42,739
也是排名蛮靠前的

305
00:13:42,739 --> 00:13:46,211
然后后边有我们要介绍要用到的

306
00:13:46,211 --> 00:13:48,391
BGE-M3和Giga-Embedding-V3

307
00:13:48,395 --> 00:13:51,572
都是在这个排名榜上

308
00:13:51,572 --> 00:13:55,455
都是相对来说有一个组织

309
00:13:57,840 --> 00:14:02,560
那么我们去看看这个FlagEmbedding这个

310
00:14:11,130 --> 00:14:13,740
就是咱们所说的这个大模型开源技术体系

311
00:14:13,750 --> 00:14:17,117
所以说我们等一下会带着大家

312
00:14:17,117 --> 00:14:21,002
做一些相关的模型家族

313
00:14:21,002 --> 00:14:24,176
其中一些嵌入模型的实战

314
00:14:24,176 --> 00:14:27,704
比如这个多语言的模型

315
00:14:27,720 --> 00:14:33,922
比如M3，比如这个多模态

316
00:14:33,930 --> 00:14:37,110
就是它还可以做一些图形的嵌入

317
00:14:37,120 --> 00:14:40,560
这里面包括覆盖面还是比较广的

318
00:14:40,560 --> 00:14:42,496
包括这个混合检索

319
00:14:42,496 --> 00:14:45,642
它里面有吸收向量的嵌入

320
00:14:45,642 --> 00:14:48,114
和密集向量的混合

321
00:14:48,638 --> 00:14:51,782
这一套家族的模型里面都有

322
00:14:51,790 --> 00:14:54,040
所以选择还是挺丰富的

323
00:14:54,040 --> 00:14:56,290
然后文档也都比较齐全

324
00:14:56,305 --> 00:15:02,425
大家可以去这里做进一步的挖掘和学习

325
00:15:04,210 --> 00:15:08,476
所以他就说B1G专注于检索增强的领域

326
00:15:08,480 --> 00:15:12,900
包括这些项目这都是他在做的一些事情

327
00:15:12,910 --> 00:15:16,790
包括Embedding模型和Rerank模型都有

328
00:15:16,795 --> 00:15:20,152
然后安装也是很简单

329
00:15:20,152 --> 00:15:23,509
就是直接安装就好了

330
00:15:23,520 --> 00:15:26,006
那如果想微调他还有教你

331
00:15:26,006 --> 00:15:29,848
怎样去安装这个可以支持微调的包

332
00:15:34,550 --> 00:15:37,371
这是BGE家族的模型的一些细节

333
00:15:37,375 --> 00:15:41,583
那我们再介绍一个Gina家族的模型

334
00:15:41,585 --> 00:15:44,273
这个Gina和BGE刚才大家也看出来了

335
00:15:44,285 --> 00:15:47,005
他们在排行榜上的排名也都蛮相似的

336
00:15:47,005 --> 00:15:52,495
然后推出的内容呢也都是有重合之处

337
00:15:52,495 --> 00:15:55,087
所以我们也可以去Gina的官网去看一下

338
00:15:55,100 --> 00:16:00,284
他给我们的一些对他自己模型的介绍

339
00:16:07,710 --> 00:16:13,416
所以Gina模型他也是提供了两个使用的方式

340
00:16:13,420 --> 00:16:17,129
一个就是你看刚才我们看到在Hugging Face上面

341
00:16:17,140 --> 00:16:21,271
你也可以去下载Gina的一系列开源模型

342
00:16:21,280 --> 00:16:23,985
也可以下载那是不是能够商用呢

343
00:16:23,990 --> 00:16:28,276
那你可要看他的模型下载之后看他的License

344
00:16:28,276 --> 00:16:32,094
他的许可证是说仅供你学习使用

345
00:16:32,094 --> 00:16:34,514
还是可以进行商业使用

346
00:16:34,520 --> 00:16:37,904
Gina应该不是一个完全开源的模型体系

347
00:16:37,910 --> 00:16:41,508
因此你要确定他的许可证

348
00:16:41,515 --> 00:16:44,431
在使用他的开源模型的时候

349
00:16:44,431 --> 00:16:47,723
但作为个人学习来讲一定是没有问题的

350
00:16:48,790 --> 00:16:53,741
那他也是推出了看在这里有一系列的Rerun重排模型

351
00:16:53,760 --> 00:16:55,687
一系列的Embedding模型

352
00:16:55,690 --> 00:16:59,738
那么如果是希望了解其中某一个模型的具体细节

353
00:16:59,740 --> 00:17:03,436
怎么样训练出来的都可以去它的文档

354
00:17:03,436 --> 00:17:07,023
和它的论文去仔细研究一下

355
00:17:07,025 --> 00:17:14,109
然后它也给出了商业许可证的使用方式

356
00:17:14,110 --> 00:17:21,135
当然也有一些模型是需要你去调用它的API才可以用的

357
00:17:21,155 --> 00:17:22,892
这个是Gina系列的模型

358
00:17:22,895 --> 00:17:26,575
这也是我们中国的公司所开发的模型

359
00:17:26,575 --> 00:17:31,647
然后另外一个比较口碑比较好的是

360
00:17:31,647 --> 00:17:35,325
这个Voyage AI推出来的一系列模型

361
00:17:35,325 --> 00:17:38,085
那这个公司应该是一个国外的公司

362
00:17:39,830 --> 00:17:41,279
我们可以去他的网站

363
00:17:41,279 --> 00:17:43,533
去看他对他这一系列模型的解释

364
00:17:43,555 --> 00:17:45,175
就不再去他的网站了

365
00:17:45,180 --> 00:17:46,503
大家可以自己去看看

366
00:17:46,510 --> 00:17:48,958
那么Cohere也是一个蛮有名的

367
00:17:48,958 --> 00:17:50,875
Embedding模型的供应商

368
00:17:50,875 --> 00:17:52,378
他也推出生成式模型

369
00:17:52,385 --> 00:17:53,766
他跟OpenAI一样

370
00:17:53,766 --> 00:17:56,756
有自己的这个大模型可以调用

371
00:17:56,765 --> 00:17:59,163
也有Embedding模型也可以用

372
00:17:59,165 --> 00:18:02,069
但是他的这个模型的水平呢

373
00:18:02,069 --> 00:18:06,124
就是可能算是处于比较靠前的位置

374
00:18:06,124 --> 00:18:07,168
但是又不拔尖

375
00:18:09,830 --> 00:18:12,274
我觉得整个没有特别突出之处

376
00:18:12,285 --> 00:18:15,264
这是Cohere这个Embedding的模型

377
00:18:15,270 --> 00:18:17,831
那还有另外一个开源模型Stella

378
00:18:17,835 --> 00:18:19,403
它的特点就是很小

379
00:18:19,405 --> 00:18:21,889
你比如说他这个400M的模型

380
00:18:21,889 --> 00:18:25,525
在测评中往往也能够达到还不错的效果

381
00:18:25,525 --> 00:18:27,605
所以就给大家一个很好的选择

382
00:18:27,615 --> 00:18:31,075
就是小而美这个模型它有这个特点

383
00:18:31,075 --> 00:18:33,295
所以在这边特别的介绍一下

384
00:18:33,295 --> 00:18:34,775
这个Stella这个模型

385
00:18:34,775 --> 00:18:36,627
它有两个版本了应该

386
00:18:36,627 --> 00:18:38,728
一个是400M一个是1.5G

387
00:18:39,930 --> 00:18:44,332
好那整体来说到底对于我们每个人来说怎么选

388
00:18:44,350 --> 00:18:47,535
就一个肯定就是网络上的口碑

389
00:18:47,540 --> 00:18:49,630
大家是不是都纷纷用这个很重要

390
00:18:49,630 --> 00:18:53,842
因为在这个时代实际上口碑还是很重要的

391
00:18:53,845 --> 00:18:56,133
然后因为大家用的人多

392
00:18:56,133 --> 00:18:59,455
这个风评好社区也支持的比较好

393
00:18:59,460 --> 00:19:01,930
它就越来越有这个优势

394
00:19:01,935 --> 00:19:04,145
否则的话一些不太知名的模型

395
00:19:04,145 --> 00:19:06,025
你用了然后过两天它也没有了

396
00:19:06,025 --> 00:19:07,590
不支持了也不太好

397
00:19:07,590 --> 00:19:09,816
所以在网络上我们去看它的这个

398
00:19:09,825 --> 00:19:11,879
这个模型的口碑还是很重要的

399
00:19:11,880 --> 00:19:13,777
那口碑怎么去看

400
00:19:13,780 --> 00:19:15,284
也给大家介绍一下

401
00:19:16,830 --> 00:19:19,008
就是我们通常是搜索

402
00:19:19,008 --> 00:19:21,670
像国外这个Reddit这个网站

403
00:19:21,670 --> 00:19:26,201
技术人员非常喜欢在这个网站搜索一些问题

404
00:19:26,210 --> 00:19:28,422
什么embedding model你们用啊

405
00:19:28,425 --> 00:19:31,353
在Reddit里面问题描述也都比较清晰

406
00:19:31,360 --> 00:19:36,159
做Reddit想问问大家都是用什么模型对吧

407
00:19:36,160 --> 00:19:38,708
然后大家就会有自己的一个看法

408
00:19:39,830 --> 00:19:45,650
比如它说Reddit它就说用这个用这个什么什么

409
00:19:47,450 --> 00:19:51,610
它回答的是VectorDB Qdrant这个不是达菲所问

410
00:19:51,610 --> 00:19:53,901
然后我们问的是embedding model嘛

411
00:19:53,910 --> 00:19:55,811
下面还有一些回答有一些

412
00:19:55,811 --> 00:20:00,151
你看这个人说的就是他说我原来是用什么什么

413
00:20:00,165 --> 00:20:04,359
后来我用BGE就说明BGE就用的人还是蛮多的

414
00:20:04,375 --> 00:20:09,380
然后他说但是BGE呢有点overfit to这个MTEB benchmark

415
00:20:09,380 --> 00:20:10,325
这话是什么意思呢

416
00:20:10,325 --> 00:20:14,201
他就说BGE可能是太过于注重按照这些benchmark

417
00:20:14,205 --> 00:20:16,218
这些数据集去做这个训练

418
00:20:16,220 --> 00:20:18,978
他可能就会有一些过拟合的现象

419
00:20:18,978 --> 00:20:21,522
就过于注重这些benchmark分数高

420
00:20:21,525 --> 00:20:24,819
但是呢实际的效果他可能他觉得并不理想

421
00:20:24,830 --> 00:20:27,908
但其实他这个评论还是得到了蛮多的支持

422
00:20:27,925 --> 00:20:30,367
因此大家可以去看一看

423
00:20:30,370 --> 00:20:34,990
有些人就会转而选择比如说千问这一系列的模型

424
00:20:34,995 --> 00:20:37,907
他就会觉得说不错是top performer

425
00:20:39,830 --> 00:20:42,802
回答也好讨论也好他也有提到Gina

426
00:20:42,805 --> 00:20:46,970
至少说明这些模型都是应景的有人用的

427
00:20:46,980 --> 00:20:49,310
也是大家所比较关注的

428
00:20:49,315 --> 00:20:56,114
当然像有一些像OmniL 6 VR这些属于是小型模型

429
00:20:56,114 --> 00:20:58,922
很基础的早期的sentence word模型

430
00:20:58,922 --> 00:21:00,121
它是属于一个benchmark

431
00:21:09,890 --> 00:21:12,313
那么你可以用它来做一个benchmark

432
00:21:12,320 --> 00:21:14,620
这些就是你的信息来源

433
00:21:14,625 --> 00:21:16,638
你可以去这些论坛看一看

434
00:21:16,645 --> 00:21:19,128
那我相信国内一定也有更多的

435
00:21:19,128 --> 00:21:21,993
这些关于国内模型的讨论的论坛

436
00:21:22,000 --> 00:21:24,142
大家都可以去看一看

437
00:21:24,145 --> 00:21:27,595
然后另外一个我是发现了一些文章

438
00:21:27,595 --> 00:21:29,950
比如我看到一篇比较好的文章

439
00:21:29,960 --> 00:21:32,290
就是最好的embedding models

440
00:21:32,290 --> 00:21:35,612
对于RAG的工作在2025年做的

441
00:21:35,615 --> 00:21:39,719
他就是对各种各样的模型做出了一个对比

442
00:21:39,725 --> 00:21:43,451
然后他这篇文章用他的方法做了比较

443
00:21:43,455 --> 00:21:46,665
他认为这个Voyager 3 Large

444
00:21:46,665 --> 00:21:49,738
是相当不错的一个模型做出来的

445
00:21:49,738 --> 00:21:54,550
后来给他的感觉其实比Gemini和Claude都要强一些

446
00:21:54,550 --> 00:21:57,743
然后这个Stable 400M在开源模型里

447
00:21:57,755 --> 00:21:58,679
他认为也是不错

448
00:21:58,680 --> 00:22:01,644
那他的测评的方法是什么呢

449
00:22:01,645 --> 00:22:03,429
大家也可以去模仿

450
00:22:03,435 --> 00:22:06,683
就是他给出了自己的一些看法

451
00:22:06,690 --> 00:22:08,230
他是自制了一些数据集

452
00:22:08,230 --> 00:22:13,810
他为了说不要让这些模型用这些公开的数据集

453
00:22:13,815 --> 00:22:17,385
因为这些公开的数据集难免会有泄露的嫌疑

454
00:22:17,395 --> 00:22:21,250
他是自己用图片去转了一些数据集

455
00:22:22,670 --> 00:22:24,884
你看他说他测了一些

456
00:22:24,884 --> 00:22:26,606
他先说这个结论

457
00:22:26,610 --> 00:22:33,255
他说这个Stella他觉得什么什么什么表现的略好

458
00:22:33,270 --> 00:22:35,526
关键是看他的测评方法

459
00:22:36,810 --> 00:22:38,218
他是找了一个图形

460
00:22:38,225 --> 00:22:39,863
他用了一个图像搜索的数据集

461
00:22:39,863 --> 00:22:42,434
然后他用这个OCR对图像数据进行了识别

462
00:22:42,435 --> 00:22:46,007
他之所以用这个数据集来做这个检测

463
00:22:46,025 --> 00:22:48,783
是因为他觉得大多数文本搜索

464
00:22:48,783 --> 00:22:51,344
数据集都已经被训练的差不多了

465
00:22:51,350 --> 00:22:52,878
就是已经都暴露了

466
00:22:52,880 --> 00:22:55,552
那么他特别选择了一个自制的数据集

467
00:22:55,560 --> 00:22:57,848
然后觉得这些是没有曝光的

468
00:22:57,855 --> 00:23:03,063
他认为这个能够提供这些模型训练者从来没见过的数据

469
00:23:03,080 --> 00:23:05,720
所以说当然你自己的每一个项目数据

470
00:23:05,735 --> 00:23:08,179
如果你已经都做了一个数据集

471
00:23:08,185 --> 00:23:09,633
当然数据集很难做

472
00:23:09,633 --> 00:23:12,529
就是你比如说你要让这个模型做分类

473
00:23:12,535 --> 00:23:15,425
你自己得先把你的这些东西都给分类好

474
00:23:15,440 --> 00:23:17,390
然后你才能够去测试模型是吧

475
00:23:17,400 --> 00:23:20,888
所以说自制数据集不是一个容易的事

476
00:23:20,895 --> 00:23:22,910
但是其实你在你做项目的时候

477
00:23:22,910 --> 00:23:25,080
即使你没有一个很庞大的数据集

478
00:23:25,105 --> 00:23:28,097
你也能够用几个数据或者几十个数据

479
00:23:28,105 --> 00:23:31,543
对你的嵌入模型也好生成模型也好的效果

480
00:23:31,560 --> 00:23:34,467
做一个最基本的感受这个你是可以有的

481
00:23:34,475 --> 00:23:38,160
所以说你做几个比较之后

482
00:23:38,165 --> 00:23:40,727
你就会知道哪个模型是真正优秀

483
00:23:40,730 --> 00:23:43,610
哪个模型是不太行你会能够感觉出来

484
00:23:43,615 --> 00:23:46,389
但是你一定要有具体的项目和任务还有目标

485
00:23:46,400 --> 00:23:47,650
就像这个人所做的一样

486
00:23:47,650 --> 00:23:49,820
他一定是把这个图像模型都转了

487
00:23:49,820 --> 00:23:52,385
然后他知道哪个图像和哪个图像相近

488
00:23:52,385 --> 00:23:54,965
然后这个A和B像这和D像

489
00:23:54,970 --> 00:23:56,680
然后他有这个评估标准

490
00:23:56,680 --> 00:23:58,732
然后他再去给这些模型打分

491
00:23:58,745 --> 00:24:01,745
他是有地方使他不是用肉眼去评分

492
00:24:01,745 --> 00:24:03,789
评出来这样的表的那是不可能的

493
00:24:03,800 --> 00:24:05,412
肉眼是评不出来这样的分数的

494
00:24:05,412 --> 00:24:07,109
这个大家理解我要说的意思是吧

495
00:24:08,850 --> 00:24:11,874
然后他有了这个评估数据集

496
00:24:11,874 --> 00:24:15,781
然后他就对标了一些成本对成本

497
00:24:15,781 --> 00:24:21,361
然后经过了这个之后他的观察就是

498
00:24:21,361 --> 00:24:26,228
他认为哪些是在这个英文上面表现好

499
00:24:26,228 --> 00:24:29,652
哪些是在其他多语言模型上面表现好

500
00:24:29,660 --> 00:24:32,210
你得用同样的这个标准做比较对吧

501
00:24:32,220 --> 00:24:35,355
然后他会觉得这个VH3 LUG

502
00:24:35,355 --> 00:24:39,590
是有明显的一个优势

503
00:24:39,590 --> 00:24:42,780
然后这个开源的模型

504
00:24:48,100 --> 00:24:53,995
也不错然后相比之下他觉得Germany

505
00:24:53,995 --> 00:24:57,748
就是性能适中但是价格低廉

506
00:24:57,748 --> 00:24:59,575
因为Germany004是免费的

507
00:24:59,575 --> 00:25:04,002
很低很低的价格达到了一个不错的效果

508
00:25:04,010 --> 00:25:07,118
然后他在这边会认为GinaV3

509
00:25:07,118 --> 00:25:10,839
和CohereV3是完全不如其他的竞争对手OK

510
00:25:10,839 --> 00:25:13,524
他这是一个有理有据的评估

511
00:25:13,535 --> 00:25:15,347
而且他是自己做了数据集的

512
00:25:15,347 --> 00:25:16,857
然后他这篇文章我觉得

老师2：
embedding模型的重要性：
1
00:00:00,000 --> 00:00:04,356
大家好在这一小节中我们来了解一下什么是Embedding

2
00:00:04,360 --> 00:00:07,792
以及它在RIG流程中的重要性

3
00:00:09,440 --> 00:00:11,280
下面这张图简要描述了

4
00:00:11,280 --> 00:00:13,120
Embedding生成的一个过程

5
00:00:13,120 --> 00:00:16,674
我们把数据对象比如说文本音频视频

6
00:00:16,674 --> 00:00:18,846
输入到Embedding模型之后

7
00:00:18,855 --> 00:00:22,471
就会得到相应的一维数值向量

8
00:00:22,480 --> 00:00:24,497
也就是说Embedding模型

9
00:00:24,497 --> 00:00:27,409
是将数据对象映射到固定大小

10
00:00:27,420 --> 00:00:31,284
且连续的一维数组的一种技术

11
00:00:31,290 --> 00:00:33,265
通常Embedding向量

12
00:00:33,265 --> 00:00:36,649
具有几千到几百的维度

13
00:00:36,655 --> 00:00:39,487
每个维度都代表着某种语义

14
00:00:39,487 --> 00:00:40,903
或者一种属性

15
00:00:45,750 --> 00:00:47,783
Embedding模型生成的

16
00:00:47,783 --> 00:00:50,070
Embedding向量中的数值

17
00:00:50,070 --> 00:00:51,764
肯定不是随机的

18
00:00:51,765 --> 00:00:53,573
这些数值需要表示

19
00:00:53,573 --> 00:00:56,511
相应数据对象的一些语义信息

20
00:00:56,515 --> 00:00:57,670
更确切地说

21
00:00:57,670 --> 00:00:59,749
在Embedding向量空间中

22
00:00:59,750 --> 00:01:02,378
语义相似的数据对象

23
00:01:02,378 --> 00:01:06,174
在向量空间中的映射距离更近

24
00:01:06,185 --> 00:01:08,705
而不相似的数据对象

25
00:01:08,705 --> 00:01:10,665
映射的距离更远

26
00:01:10,680 --> 00:01:14,820
这个也是向量模型需要学习的目标

27
00:01:14,830 --> 00:01:16,886
上面这些图展示了

28
00:01:16,886 --> 00:01:19,970
不同的数据在向量空间中的

29
00:01:19,970 --> 00:01:21,512
一个位置关系

30
00:01:21,535 --> 00:01:24,547
比如右边这个图描述天气的

31
00:01:24,547 --> 00:01:29,050
不同的Embedding向量在空间中会更加靠近

32
00:01:29,050 --> 00:01:33,291
而这些Embedding向量会远离描述水果的

33
00:01:33,291 --> 00:01:36,161
Embedding向量从这个空间位置上

34
00:01:36,161 --> 00:01:40,601
就可以看出这两类词汇在语义上的相关性

35
00:01:44,230 --> 00:01:47,470
我们知道在RIG的流程中

36
00:01:47,470 --> 00:01:50,990
RIG需要在外部知识库中去检索

37
00:01:50,990 --> 00:01:54,630
和所提出问题相关的一些文档

38
00:01:54,630 --> 00:01:59,778
这些文档被用来增加问题的上下文信息

39
00:01:59,790 --> 00:02:01,068
那么是如何提出问题

40
00:02:01,070 --> 00:02:03,462
如何来选择这些上下文信息呢

41
00:02:04,890 --> 00:02:08,622
就是通过计算问题的Embedding

42
00:02:08,622 --> 00:02:11,900
跟文档的Embedding的相似性

43
00:02:11,900 --> 00:02:16,128
来选择跟问题Embedding越相似的文档

44
00:02:16,128 --> 00:02:20,034
作为它的相关的一个上下文信息

45
00:02:20,035 --> 00:02:22,036
因此Embedding的好坏

46
00:02:22,036 --> 00:02:26,210
就直接关系到能否选择到相关的上下文

47
00:02:28,270 --> 00:02:31,210
如果引入了不相关的上下文

48
00:02:31,210 --> 00:02:35,310
就会对大语言模型的回答造成一定的干扰


embedding是怎么炼成的？：
1
00:00:00,000 --> 00:00:02,170
这小节我们来了解一下

2
00:00:02,170 --> 00:00:04,993
Embedding模型是如何训练出来的

3
00:00:05,010 --> 00:00:06,992
现在主流的Embedding模型

4
00:00:06,992 --> 00:00:08,576
大部分是BERT架构的

5
00:00:08,590 --> 00:00:11,575
还记得我们之前讲过Transformer架构吗

6
00:00:11,575 --> 00:00:15,034
BERT也是一个典型的Transformer架构

7
00:00:15,040 --> 00:00:17,360
具体的是说BERT它是

8
00:00:17,360 --> 00:00:20,214
Transformer中的编码器部分

9
00:00:20,215 --> 00:00:22,783
最早是谷歌在2018年提出来的

10
00:00:22,790 --> 00:00:26,166
BERT的目的是在大量的无监督数据里面

11
00:00:26,166 --> 00:00:28,276
通过双向的注意力机制

12
00:00:28,300 --> 00:00:31,745
来更好地提取文本的语义信息

13
00:00:31,755 --> 00:00:34,251
具体的训练过程分为两个部分

14
00:00:34,255 --> 00:00:37,258
第一部分我们称之为掩码填空

15
00:00:37,265 --> 00:00:39,425
它的目的是来学习词语

16
00:00:39,425 --> 00:00:42,233
在不同上下文之间的语义

17
00:00:42,250 --> 00:00:44,620
另外一个叫预测下一句

18
00:00:44,620 --> 00:00:47,150
主要是来学习字字之间的

19
00:00:47,150 --> 00:00:49,450
语义的相关性

20
00:00:49,450 --> 00:00:51,858
接下来我们通过一个简单的例子

21
00:00:51,858 --> 00:00:53,234
来说明这两个部分

22
00:00:53,255 --> 00:00:55,313
首先是文本填空

23
00:00:55,315 --> 00:01:00,194
顾名思义就是学习如何填充缺失的信息

24
00:01:00,195 --> 00:01:02,565
比如给定一个训练文本

25
00:01:02,565 --> 00:01:03,987
包含四个token

26
00:01:03,990 --> 00:01:06,707
然后用一个特殊的标记mask

27
00:01:06,707 --> 00:01:08,931
挖掉其中的一个token

28
00:01:08,940 --> 00:01:11,800
比如这里挖掉的是气这个token

29
00:01:11,800 --> 00:01:14,200
并以此作为模型的一个输入

30
00:01:14,200 --> 00:01:16,750
而模型的目的就是预测

31
00:01:16,750 --> 00:01:18,791
这个被mask掉的token

32
00:01:18,805 --> 00:01:21,046
这里指的是'气'这个token

33
00:01:21,050 --> 00:01:24,102
右边这张图详细说明了这个过程

34
00:01:24,115 --> 00:01:26,919
首先把'气'用mask遮住

35
00:01:26,920 --> 00:01:28,517
然后这四个token

36
00:01:28,517 --> 00:01:30,798
其中一个是mask掉的token

37
00:01:30,805 --> 00:01:32,485
直接输入到BERT中

38
00:01:32,490 --> 00:01:36,450
BERT的编码器会输出四个向量

39
00:01:36,455 --> 00:01:40,199
然后在mask掉的这个位置的向量

40
00:01:40,199 --> 00:01:42,503
后面加一个分类层

41
00:01:42,515 --> 00:01:45,083
通常是一个线性层

42
00:01:45,085 --> 00:01:48,935
然后是一个规划的softmax层

43
00:01:48,935 --> 00:01:51,665
这个分类器的作用就是来预测

44
00:01:51,665 --> 00:01:54,524
被mask掉的那个token的一个概率

45
00:01:54,525 --> 00:01:57,974
这样就完成了整个网络填空的一个训练

46
00:01:57,975 --> 00:02:01,575
网络填空的优势就是网络上任何一段文本

47
00:02:01,575 --> 00:02:04,230
都是可以用来作为训练的语料

48
00:02:04,230 --> 00:02:06,667
因为你只要mask掉其中的token

49
00:02:06,675 --> 00:02:09,349
而把剩下的token作为输入

50
00:02:09,355 --> 00:02:13,149
而mask掉那个token作为学习的目标

51
00:02:13,150 --> 00:02:17,710
这样就可以收集大量的训练数据来训练模型

52
00:02:17,725 --> 00:02:20,260
而数据越多模型的性能就越好

53
00:02:20,270 --> 00:02:24,010
最重要的是这整个过程都不需要人工进行参与

54
00:02:24,010 --> 00:02:26,992
也就是说不需要人工进行打标

55
00:02:26,995 --> 00:02:30,640
BERT训练的第二个部分是预测下一句

56
00:02:30,640 --> 00:02:31,920
它的思路也很简单

57
00:02:31,920 --> 00:02:35,014
就是预测输入模型的两个序列

58
00:02:35,014 --> 00:02:37,394
是否在语义上是连续的

59
00:02:37,405 --> 00:02:40,189
所以说这是一个典型的二分类的问题

60
00:02:40,190 --> 00:02:42,892
那怎么来构造这个训练数据集呢

61
00:02:42,900 --> 00:02:45,220
首先给定两个训练文本

62
00:02:45,220 --> 00:02:49,880
这两个训练文本每个文本是由两个序列组成的

63
00:02:49,880 --> 00:02:52,432
很显然第一句的两个序列

64
00:02:52,432 --> 00:02:54,520
它的语意上是连续的

65
00:02:54,520 --> 00:02:58,376
而第二句两个序列语意上是不连续的

66
00:02:58,385 --> 00:03:01,479
然后为了区分不同的输入序列

67
00:03:01,485 --> 00:03:07,376
在两个序列之间用一个特殊的Token SEP来分割

68
00:03:07,390 --> 00:03:09,466
然后因为它是一个分类问题

69
00:03:09,475 --> 00:03:16,296
为了做二分类还需要增加一个特殊的分类的Token CLS

70
00:03:16,310 --> 00:03:18,425
有了这样的设定以后

71
00:03:18,425 --> 00:03:23,945
我们来看一下给定序列是如何构造模型的输入和输出

72
00:03:25,220 --> 00:03:27,794
比如这里的第一句话

73
00:03:27,795 --> 00:03:30,850
今天天气如何很好

74
00:03:30,850 --> 00:03:33,742
这两个序列正如前面所说的

75
00:03:33,750 --> 00:03:37,711
在开头加入分类的Token CLS

76
00:03:37,720 --> 00:03:43,058
在两个序列中间加入序列分割的Token SEP

77
00:03:43,065 --> 00:03:45,375
以此作为模型的一个输入

78
00:03:45,375 --> 00:03:49,080
因为这句话明显是语义连续的

79
00:03:49,085 --> 00:03:51,879
所以说它的预测目标是YES

80
00:03:51,885 --> 00:03:53,352
同样的对另外一句话

81
00:03:53,360 --> 00:03:57,872
我们也会加入分类的Token CLS

82
00:03:57,875 --> 00:04:01,593
跟序列之间的分割Token SEP

83
00:04:01,595 --> 00:04:04,168
第二句话这个是语义不连续的

84
00:04:04,170 --> 00:04:06,062
所以我们预测的目标是NO

85
00:04:06,070 --> 00:04:08,295
我们上面所说过预测下一句

86
00:04:08,295 --> 00:04:11,991
它的目的是学习两个句子之间的

87
00:04:11,991 --> 00:04:14,103
语义的一个关联性

88
00:04:14,105 --> 00:04:17,947
那这个数据构造的过程需要人工标记吗

89
00:04:19,440 --> 00:04:21,175
同样的它也是不需要的

90
00:04:21,180 --> 00:04:22,863
对于语义连续的样本

91
00:04:22,870 --> 00:04:26,646
我们可以从网络中随意的抽出一段话

92
00:04:26,660 --> 00:04:31,080
截取连续的两句话就认为是语义连续的

93
00:04:31,085 --> 00:04:33,315
对于语义不连续的样本

94
00:04:33,315 --> 00:04:36,195
可以通过不同类型的文档中

95
00:04:36,195 --> 00:04:39,315
随机抽取两个部分来进行构建

96
00:04:39,330 --> 00:04:42,285
当然这个需要做一定的数据预处理

97
00:04:42,285 --> 00:04:46,080
来保证生成的训练样本的质量

98
00:04:46,090 --> 00:04:49,174
经过完形填空和预测下一句

99
00:04:49,185 --> 00:04:52,160
这两个部分的训练策略

100
00:04:52,160 --> 00:04:55,408
这种模型就会为每个输入的token

101
00:04:55,408 --> 00:04:57,560
生成一个embedding向量

102
00:04:57,560 --> 00:05:00,202
这些embedding向量既体现了

103
00:05:00,202 --> 00:05:03,880
每个token所在上下文信息的一个语义

104
00:05:03,885 --> 00:05:07,830
也体现了句子之间的一个语义信息


主流中文embedding模型：
1
00:00:00,000 --> 00:00:02,856
大家好这一小节我们来介绍一下

2
00:00:02,856 --> 00:00:05,150
常用的中文Embedding模型

3
00:00:05,150 --> 00:00:07,330
首先我们先来介绍一个

4
00:00:07,330 --> 00:00:10,382
针对向量模型的基准测试

5
00:00:10,395 --> 00:00:13,779
这个基准测试的名称叫做MTEB

6
00:00:13,790 --> 00:00:17,438
MTEB提供了不同Embedding任务的

7
00:00:17,438 --> 00:00:20,495
多种基准的评测数据集

8
00:00:20,495 --> 00:00:24,178
针对每个任务不同的Embedding模型

9
00:00:24,178 --> 00:00:26,670
在MTEB测试集上进行测试

10
00:00:26,670 --> 00:00:29,712
同时对这些测试结果进行排名

11
00:00:29,720 --> 00:00:30,966
组成一个排行榜

12
00:00:30,970 --> 00:00:34,966
比如这张图就展示了检索这个任务下面的

13
00:00:34,966 --> 00:00:38,415
中文的Embedding模型的一个排名情况

14
00:00:38,420 --> 00:00:40,580
在MTEB上我们可以看到

15
00:00:40,580 --> 00:00:43,980
很多不同类型的Embedding模型

16
00:00:43,980 --> 00:00:48,769
除了我们上面讲过的BERT架构的Embedding模型

17
00:00:48,775 --> 00:00:53,449
最近也出了很多基于大语言模型的Embedding模型

18
00:00:53,450 --> 00:00:54,728
这里我们主要讲两种

19
00:00:54,728 --> 00:00:56,574
应用中比较常见的Embedding模型

20
00:00:56,580 --> 00:01:00,570
他们都是BERT架构的一个改进版本

21
00:01:00,580 --> 00:01:02,846
第一个是阿里巴巴提出的

22
00:01:02,846 --> 00:01:04,701
GTE系列的Embedding模型

23
00:01:04,710 --> 00:01:08,346
GTE全称是通用文本嵌入模型

24
00:01:08,350 --> 00:01:10,366
和原始的BERT不同的是

25
00:01:10,366 --> 00:01:12,158
GTE加入了度量学习

26
00:01:12,158 --> 00:01:14,300
和对比学习两个过程

27
00:01:14,300 --> 00:01:16,940
我们先来看一下度量学习

28
00:01:16,945 --> 00:01:19,706
首先对于相关的两个句子

29
00:01:19,706 --> 00:01:21,212
比如说A和B

30
00:01:22,320 --> 00:01:24,849
将A和B这两个句子

31
00:01:24,849 --> 00:01:27,818
同时输入到BiRD模型里面去

32
00:01:27,820 --> 00:01:29,890
我们知道BiRD模型会为

33
00:01:29,890 --> 00:01:32,881
输入的每一个Token生成这个

34
00:01:32,900 --> 00:01:35,899
Token对应的Embedding向量度量学习

35
00:01:35,899 --> 00:01:39,518
这边会将所有Token对应的向量求一个均值

36
00:01:39,535 --> 00:01:41,695
对应到这里的一个Pooling操作

37
00:01:41,695 --> 00:01:43,400
经过Pooling操作以后

38
00:01:43,400 --> 00:01:47,875
就会得到这个句子对应的句子级别的Embedding向量

39
00:01:47,875 --> 00:01:51,589
然后针对两个句子级别的Embedding向量

40
00:01:51,589 --> 00:01:53,625
计算余弦相似度距离

41
00:01:53,625 --> 00:01:56,880
这个就完成了一次度量学习的过程

42
00:01:56,885 --> 00:01:59,515
而度量学习的目标就是

43
00:01:59,515 --> 00:02:04,249
相关的两个句子的语义相似度尽可能为1

44
00:02:04,255 --> 00:02:08,275
而不相关的两个句子的语义相似度尽可能为0

45
00:02:08,275 --> 00:02:09,835
经过这个学习目标

46
00:02:09,835 --> 00:02:12,760
就完成了一次度量学习的过程

47
00:02:12,780 --> 00:02:16,294
右边这个图展示GTE在构造度量学习

48
00:02:16,294 --> 00:02:19,306
中所运用的数据的一个过程

49
00:02:19,330 --> 00:02:22,526
收集的都是不同类型的配对数据

50
00:02:22,540 --> 00:02:24,190
比如说新闻的数据

51
00:02:24,190 --> 00:02:26,500
就是将新闻的摘要和新闻的资料

52
00:02:26,520 --> 00:02:29,220
作为一个配对的相关的文本

53
00:02:29,220 --> 00:02:31,920
作为一个训练数据进行训练

54
00:02:31,920 --> 00:02:33,972
另外一个比如像代码

55
00:02:33,972 --> 00:02:38,304
就是将代码里面的注释和注释所对应的代码

56
00:02:38,310 --> 00:02:40,444
作为一个相关的配对数据

57
00:02:40,444 --> 00:02:43,548
也作为训练数据输入到模型进行训练

58
00:02:43,570 --> 00:02:48,130
这些收集到的数据类型质量都是相对比较高的

59
00:02:48,130 --> 00:02:51,210
而数据质量对于模型性能的好坏

60
00:02:51,210 --> 00:02:53,410
是一个关键的一个因素

61
00:02:53,415 --> 00:02:56,418
另外一种学习方式是对比学习

62
00:02:58,260 --> 00:03:03,010
对比学习是通过构建相关与不相关的三元组

63
00:03:03,010 --> 00:03:04,510
来进行学习的

64
00:03:04,515 --> 00:03:09,815
它的思路就是让相关的两个文本的embedding越接近

65
00:03:09,820 --> 00:03:13,825
而不相关的两个文本的embedding越远离

66
00:03:13,825 --> 00:03:17,997
具体的过程是先构造一个三元组

67
00:03:17,997 --> 00:03:20,083
比如说叫QA和I

68
00:03:20,100 --> 00:03:23,572
其中Q跟A这两个文本是相关的

69
00:03:23,572 --> 00:03:27,044
而Q和I这两个文本是不相关的

70
00:03:27,060 --> 00:03:36,012
对比学习的目标就是使得相关的两个文本Q和A的embedding

71
00:03:36,020 --> 00:03:37,436
距离尽可能小

72
00:03:40,000 --> 00:03:45,209
而不相关的两个文本Q和I的embedding距离尽可能大

73
00:03:45,225 --> 00:03:48,347
这种对比学习的方式其实很符合

74
00:03:48,347 --> 00:03:51,700
我们最初对于embedding模型的一个定义

75
00:03:51,700 --> 00:03:54,879
另外一个值得注意的是这里的对比学习

76
00:03:54,879 --> 00:03:58,855
是在有标注的数据上进行微调的

77
00:03:58,855 --> 00:04:01,737
通过对比学习和度量学习

78
00:04:01,737 --> 00:04:04,619
然后在很大量的数据上

79
00:04:04,625 --> 00:04:09,026
GTE就训练出了一个比较好的embedding模型

80
00:04:09,040 --> 00:04:13,705
另外一个embedding模型是智源人工智能研究院提出的BGE

81
00:04:13,720 --> 00:04:15,218
系列的embedding模型

82
00:04:16,700 --> 00:04:19,351
其中比较新的模型是M3

83
00:04:19,355 --> 00:04:22,103
M3由三个重要的功能组成

84
00:04:22,105 --> 00:04:24,495
第一个是它支持多语言

85
00:04:24,500 --> 00:04:28,448
第二个是它支持不同类型的检索

86
00:04:28,448 --> 00:04:32,971
包括密集检索、稀疏检索还有多向量检索

87
00:04:32,980 --> 00:04:36,676
第三个是它支持不同的文本粒度

88
00:04:36,676 --> 00:04:42,160
比如说字级别的、段落级别的还有文档级别的

89
00:04:42,170 --> 00:04:44,600
M3综合了这三个功能

90
00:04:44,600 --> 00:04:47,780
是当前比较强的一个embedding模型

91
00:04:49,660 --> 00:04:51,892
M3的一个大的失误

92
00:04:51,892 --> 00:04:55,120
和前面提到的GTE系列是类似的

93
00:04:55,120 --> 00:04:59,585
也是通过度量学习和对比学习的方式来训练embedding模型

94
00:04:59,585 --> 00:05:02,469
只不过其中的一些细节有所不同

95
00:05:02,475 --> 00:05:05,610
第一个不同是度量学习中的BERT模型

96
00:05:05,610 --> 00:05:10,619
M3采用的是BERT的一个变种叫LoftTA

97
00:05:10,620 --> 00:05:13,602
另外一个不同是在计算相似度时

98
00:05:13,615 --> 00:05:17,413
M3采用的是多种维度的相似度计算

99
00:05:17,430 --> 00:05:20,749
其中包括Dense密集检索的相似度

100
00:05:20,760 --> 00:05:22,880
以及稀疏检索的相似度

101
00:05:22,880 --> 00:05:26,060
还有就是多向量检索的一个相似度

102
00:05:29,020 --> 00:05:31,542
其中这里的Dense就是密集检索

103
00:05:31,545 --> 00:05:33,729
和GTE中是一样的

104
00:05:33,730 --> 00:05:37,487
也就是所有的BERT输出的token的一个均值

105
00:05:37,495 --> 00:05:42,068
而稀疏检索就是最基本的文本字符串匹配

106
00:05:42,080 --> 00:05:44,759
而多向量检索是将所有的token

107
00:05:44,759 --> 00:05:46,819
映射到一个多维的向量

108
00:05:46,835 --> 00:05:50,609
然后在这个多维向量上进行计算相似度

109
00:05:50,620 --> 00:05:56,236
这样做的好处是能够从局部更细粒度的进行语义的检索

110
00:05:56,255 --> 00:06:00,523
换句话说密集检索其实是把所有的token

111
00:06:00,523 --> 00:06:02,531
映射到一个维度上

112
00:06:02,545 --> 00:06:06,070
Dense是从全局的角度来考虑相似度

113
00:06:06,075 --> 00:06:11,178
而多项量检索其实是从更多维度上去考虑相似度

114
00:06:11,185 --> 00:06:15,593
你可以认为M3模型是一个从总体到局部

115
00:06:15,600 --> 00:06:20,532
从语义到文本匹配的一个混合的检索模型

116
00:06:20,540 --> 00:06:20,540
相信对于这类检索有所优势


embedding模型排行榜靠谱不靠谱，如何选择：
1
00:00:00,000 --> 00:00:02,192
这一小节我们来回答一个重要的问题

2
00:00:02,195 --> 00:00:06,211
如何为RAG挑选一个合适的embedding模型

3
00:00:06,215 --> 00:00:07,439
那从何开始呢

4
00:00:07,440 --> 00:00:12,995
我们前面说过一个文本embedding的一个基准测试板单MTEB

5
00:00:12,995 --> 00:00:16,140
这个榜单除了提供不同的基准测试任务

6
00:00:16,145 --> 00:00:19,297
还展示了不同embedding模型的一些属性

7
00:00:19,300 --> 00:00:21,221
比如embedding模型的大小

8
00:00:21,225 --> 00:00:23,605
支持的最大token数等等

9
00:00:23,605 --> 00:00:27,637
这为我们了解一个embedding模型提供了有用的信息

10
00:00:27,640 --> 00:00:32,120
因此从MTEB板单开始来挑选embedding模型

11
00:00:32,120 --> 00:00:33,812
是一个好的开始

12
00:00:33,815 --> 00:00:37,289
那我们应该关注板单上的哪些重要信息呢

13
00:00:38,680 --> 00:00:40,070
首先是任务

14
00:00:40,070 --> 00:00:42,807
embedding模型可以运营很多不同的任务

15
00:00:42,815 --> 00:00:46,026
比如说分类的任务、聚类的任务

16
00:00:46,026 --> 00:00:48,980
检索任务、总结等不同的任务

17
00:00:48,985 --> 00:00:52,177
当然RAG应用是重在检索信息

18
00:00:52,185 --> 00:00:56,194
所以我们可以选择Retrieval检索任务下面的embedding模型

19
00:00:56,195 --> 00:00:57,537
第二个是embedding模型

20
00:00:57,540 --> 00:00:58,661
这个是embedding模型所支持的语言

21
00:00:58,665 --> 00:01:02,303
这个和embedding模型训练的语料是有关系的

22
00:01:02,315 --> 00:01:05,885
如果你的模型是纯英文语料训练出来的

23
00:01:05,900 --> 00:01:09,970
那么大概率在中文的检索中它的表现是十分糟糕的

24
00:01:09,990 --> 00:01:13,543
所以要根据你的任务的数据来选择模型

25
00:01:13,555 --> 00:01:14,976
如果是中文语境

26
00:01:14,976 --> 00:01:17,209
建议是选择多语言的支持

27
00:01:17,210 --> 00:01:19,058
特别是中文和英文的支持

28
00:01:20,140 --> 00:01:25,099
第三个是模型在MTEB测试集上的一个测试得分

29
00:01:25,100 --> 00:01:27,515
这个得分一定程度上可以反映到MTEB

30
00:01:27,520 --> 00:01:30,200
一般可以选择名次靠前

31
00:01:30,200 --> 00:01:35,019
而且发布机构是比较有名的一个embedding模型

32
00:01:35,020 --> 00:01:37,228
第四个是要考虑模型的大小

33
00:01:37,235 --> 00:01:41,160
这个关系让你部署embedding模型的设备资源

34
00:01:41,160 --> 00:01:43,595
和对执行性能的一个要求

35
00:01:43,595 --> 00:01:46,442
越大的模型需要的资源就越多

36
00:01:46,442 --> 00:01:48,194
并且延迟也会很高

37
00:01:48,210 --> 00:01:52,458
这里不太建议选择大模型微调后的一个embedding模型

38
00:01:52,475 --> 00:01:55,177
虽然它在得分上是比较有优势的

39
00:01:55,177 --> 00:02:00,653
但是和那些不低价格的模型差距并不是十分明显

40
00:02:00,653 --> 00:02:03,908
而它耗费的部署资源是十分巨大的

41
00:02:03,920 --> 00:02:06,428
第五个是embedding模型的维度

42
00:02:06,430 --> 00:02:09,980
也就是输出embedding向量的长度

43
00:02:09,985 --> 00:02:15,126
embedding维度长的好处是可以捕捉更复杂的语义

44
00:02:15,135 --> 00:02:19,612
而embedding模型维度小它的执行性能就更好

45
00:02:19,612 --> 00:02:21,250
也更易于存储

46
00:02:21,255 --> 00:02:23,619
所以说在这里要取一个折中

47
00:02:23,619 --> 00:02:25,195
一般是不需要太长

48
00:02:25,195 --> 00:02:30,043
现在的语义模型的embedding维度一般是在512或者1024

49
00:02:30,050 --> 00:02:32,991
第六个是输入的最大token数

50
00:02:33,000 --> 00:02:37,180
这个关系到你输入到embedding模型的最大token数

51
00:02:37,180 --> 00:02:41,920
如果说超过最大的token数，模型就会自动做截断

52
00:02:41,925 --> 00:02:46,473
所以需要在输入embedding时不要超过这个最大的token数

53
00:02:46,480 --> 00:02:50,110
这个也是后面RIG在进行文本分割时

54
00:02:50,115 --> 00:02:52,524
需要重点考虑的一个参数

55
00:02:52,530 --> 00:02:58,390
当然需要注意的是MTEB的评测并不总是这个

56
00:02:58,390 --> 00:03:01,610
和前面讲过的大圆模型的各种评

57
00:03:01,610 --> 00:03:04,370
测榜单的问题其实是一样的

58
00:03:04,370 --> 00:03:07,032
因为MTEB的评测集是公开的

59
00:03:07,032 --> 00:03:11,000
所以说就有可能有一些水分在里面

60
00:03:11,000 --> 00:03:14,016
我们可以结合华布模型的机构

61
00:03:14,016 --> 00:03:19,231
以及上面所说的几个维度对模型进行一个初步的筛选

62
00:03:19,235 --> 00:03:24,760
这个模型筛选的过程是可以在后面进行逐步动态调整的

63
00:03:25,880 --> 00:03:32,222
也就是说我们会根据上面提出的几个关注的维度

64
00:03:32,222 --> 00:03:34,898
做一个模型的初步筛选

65
00:03:36,180 --> 00:03:37,015
然后把这个

66
00:03:37,015 --> 00:03:40,820
初步筛选的模型作为一个baseline的模型基准模型

67
00:03:44,400 --> 00:03:49,169
然后通过构建自己的一个数据集来进行测试

68
00:03:52,020 --> 00:03:53,145
然后根据测

69
00:03:53,145 --> 00:03:58,040
试的结果来判断这个baseline的模型是不是符合你的要求

70
00:03:58,040 --> 00:04:04,400
如果不符合要求那我们需要重新选择模型进行重新测试

71
00:04:06,060 --> 00:04:10,860
通过几轮迭代就可以选出你心仪的Embedding模型吧

72
00:04:10,860 --> 00:04:11,135
所以这是最优的方式了
