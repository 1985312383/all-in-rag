# 第二节 文本分块

## 一、什么是文本分块

文本分块（Text Chunking）是构建RAG流程的关键步骤。它的核心原理是将加载后的长篇文档，切分成更小、更易于处理的单元。这些被切分出的文本块，是后续向量检索和模型处理的**基本单位**。

![文本分块示意图](./images/2_1.webp)

## 二、为何需要文本分块

将长文本分解为适当大小的片段，主要基于两大考量：模型的**上下文限制**和检索生成的**性能需求**。

### 2.1 满足模型上下文限制

将文本分块的首要原因，是为了适应RAG系统中两个核心组件的硬性限制：

-   **嵌入模型 (Embedding Model)**: 负责将文本块转换为向量。这类模型有严格的输入长度上限。例如，许多常用的嵌入模型（如 `bge-base-zh-v1.5`）的上下文窗口为512个token。任何超出此限制的文本块在输入时都会被截断，导致信息丢失，生成的向量也无法完整代表原文的语义。因此，文本块的大小**必须**小于等于嵌入模型的上下文窗口。

-   **大语言模型 (LLM)**: 负责根据检索到的上下文生成答案。LLM同样有上下文窗口限制（尽管通常比嵌入模型大得多，从几千到上百万token不等）。检索到的所有文本块，连同用户问题和提示词，都必须能被放入这个窗口中。如果单个块过大，可能会导致只能容纳少数几个相关的块，限制了LLM回答问题时可参考的信息广度。

因此，分块是确保文本能够被两个模型完整、有效处理的基础。

### 2.2 为何“块”不是越大越好

假设嵌入模型最多能处理8192个token，是否应该把块切得尽可能大（比如8000个token）呢？答案是否定的。**块的大小并非越大越好**，过大的块会严重影响RAG系统的性能。

#### 2.2.1 嵌入过程中的信息损失

大多数嵌入模型都基于 Transformer 编码器。其工作流程大致如下：

1.  **分词 (Tokenization)**: 将输入的文本块分解成一个个 token。
2.  **向量化 (Vectorization)**: Transformer 为**每个 token** 生成一个高维向量表示。
3.  **池化 (Pooling)**: 通过某种策略（如取`[CLS]`[^1]位的向量、对所有token向量求平均`mean pooling`等），将所有 token 的向量**压缩**成一个**单一的向量**，这个向量代表了整个文本块的语义。

在这个`压缩`过程中，信息损失是不可避免的。一个768维的向量需要概括整个文本块的所有信息。**文本块越长，包含的语义点越多，这个单一向量所承载的信息就越稀释**，导致其表示变得笼统，关键细节被模糊化，从而降低了检索的精度。

#### 2.2.2 生成过程的“大海捞针” (Lost in the Middle)

即使将检索到的多个大块文本都塞进LLM的长上下文窗口中，也会出现关键信息被“淹没”在大量无关内容里的问题。有研究表明[^2]，当LLM处理非常长的、充满大量信息的上下文时，它倾向于更好地记住开头和结尾的信息，而忽略中间部分的内容。

如果提供给LLM的上下文块又大又杂，充满了与问题无关的噪音，模型就很难从中提取出最关键的信息来形成答案，从而导致回答质量下降或产生幻觉。

#### 2.2.3 主题稀释导致检索失败

一个好的文本块应该聚焦于一个明确、单一的主题。如果一个块包含太多不相关的主题，它的语义就会被稀释，导致在检索时无法被精确匹配。

**举个栗子🌰：**

假设有一个关于《王者荣耀》英雄鲁班七号的攻略文档。

- **糟糕的分块策略**：将“技能介绍”、“推荐出装”和“背景故事”这三个完全不同主题的内容，全部放在一个巨大的文本块里。
    - 当玩家查询“鲁班七号怎么出装？”时，这个大块虽然包含了出装信息，但由于被技能说明和英雄故事等无关主题严重稀释，其整体的检索相关性得分可能会很低，导致无法被召回。

- **优秀的分块策略**：将“技能”、“出装”和“故事”分别切分为三个独立的、主题聚焦的块。
    - 当玩家再次查询时，“推荐出装”这个块会因为与查询高度相关而获得极高的分数，从而被精准地检索出来。

通过合理分块，可以有效提升检索的信噪比，确保了后续生成环节能得到最优质、最相关的上下文。

## 三、主流分块策略

LangChain 提供了丰富且易于使用的文本分割器（Text Splitters），下面将介绍几种最核心的策略。

### 3.1 固定大小分块

这是最简单直接的分块方法。根据LangChain源码，它的工作原理分为两个核心阶段：

1.  **按段落分割**：`CharacterTextSplitter` 采用默认分隔符 `"\n\n"`，使用正则表达式将文本按段落进行分割，通过 `_split_text_with_regex` 函数处理。

2.  **智能合并**：调用继承自父类的 `_merge_splits` 方法，将分割后的段落依次合并。该方法会监控累积长度，当超过 `chunk_size` 时形成新块，并通过重叠机制（`chunk_overlap`）保持上下文连续性，同时在必要时发出超长块的警告。

需要注意的是，`CharacterTextSplitter` 实际实现的并非严格的固定大小分块。根据 `_merge_splits` 源码逻辑，它会：

- **优先保持段落完整性**：只有当添加新段落会导致总长度超过 `chunk_size` 时，才会结束当前块
- **处理超长段落**：如果单个段落超过 `chunk_size`，系统会发出警告但仍将其作为完整块保留
- **应用重叠策略**：通过 `chunk_overlap` 参数在块之间保持内容重叠，确保上下文连续性

因此，LangChain的实现更准确地应该称为"段落感知的自适应分块"，块大小会根据段落边界动态调整。

下面的代码展示了如何配置一个固定大小分块器：

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

loader = TextLoader("../../data/C2/txt/蜂医.txt")
docs = loader.load()

text_splitter = CharacterTextSplitter(
    chunk_size=100,    # 每个块的目标大小为100个字符
    chunk_overlap=10   # 每个块之间重叠10个字符，以缓解语义割裂
)

chunks = text_splitter.split_documents(docs)

print(f"文本被切分为 {len(chunks)} 个块。\n")
print("--- 前5个块内容示例 ---")
for i, chunk in enumerate(chunks[:5]):
    print("=" * 60)
    # chunk 是一个 Document 对象，需要访问它的 .page_content 属性来获取文本
    print(f'块 {i+1} (长度: {len(chunk.page_content)}): "{chunk.page_content}"')
```

这种方法的主要优势在于实现简单、处理速度快且计算开销小。劣势在于可能会在语义边界处切断文本，影响内容的完整性和连贯性。需要注意的是，实际的固定大小分块实现（如LangChain的 `CharacterTextSplitter`）通常会结合分隔符来减少这种问题，在段落边界处优先切分，只有在必要时才会强制按大小切断。因此，这种方法在日志分析、数据预处理等场景中仍有其应用价值。

### 3.2 递归字符分块

在前面的章节中，已经使用了 `RecursiveCharacterTextSplitter()` 的默认配置来处理文档分块。现在让我们深入了解 `RecursiveCharacterTextSplitter()` 的实现。它通过分隔符层级递归处理，解决了固定大小分块无法处理超长文本的问题。

**算法流程**：
1.  **寻找有效分隔符**: 从分隔符列表中从前到后遍历，找到第一个在当前文本中**存在**的分隔符。如果都不存在，使用最后一个分隔符（通常是空字符串 `""`）。
2.  **切分与分类处理**: 使用选定的分隔符切分文本，然后遍历所有片段：
    *   **如果片段不超过块大小**: 暂存到 `_good_splits` 中，准备合并
    *   **如果片段超过块大小**:
        *   首先，将暂存的合格片段通过 `_merge_splits` 合并成块
        *   然后，检查是否还有剩余分隔符：
            - **有剩余分隔符**: 递归调用 `_split_text` 继续分割
            - **无剩余分隔符**: 直接保留为超长块
3.  **最终处理**: 将剩余的暂存片段合并成最后的块

**实现细节**：
- **批处理机制**: 先收集所有合格片段（`_good_splits`），遇到超长片段时才触发合并操作。
- **递归终止条件**: 关键在于 `if not new_separators` 判断。当分隔符用尽时（`new_separators` 为空），停止递归，直接保留超长片段。确保算法不会无限递归。

**与固定大小分块的关键差异**：
- 固定大小分块遇到超长段落时只能发出警告并保留
- 递归分块会继续使用更细粒度的分隔符（句子→单词→字符）直到满足大小要求

具体示例如下：

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 加载文档为 `docs`

text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", "。", "，", " ", ""],  # 分隔符优先级
    chunk_size=200,
    chunk_overlap=10,
)

chunks = text_splitter.split_text(docs)
```

**分隔符策略**：
- **默认分隔符**：`["\n\n", "\n", " ", ""]`
- **多语言支持**：对于无词边界语言（中文、日文、泰文），可添加：
  ```python
  separators=[
      "\n\n", "\n", " ",
      ".", ",", "\u200b",      # 零宽空格(泰文、日文)
      "\uff0c", "\u3001",      # 全角逗号、表意逗号
      "\uff0e", "\u3002",      # 全角句号、表意句号
      ""
  ]
  ```

**编程语言特化支持**：
`RecursiveCharacterTextSplitter` 能够针对特定的编程语言（如Python, Java等）使用预设的、更符合代码结构的分隔符。它们通常包含语言的顶级语法结构（如类、函数定义）和次级结构（如控制流语句），以实现更符合代码逻辑的分割。
```python
# 针对代码文档的优化分隔符
splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,  # 支持Python、Java、C++等
    chunk_size=500,
    chunk_overlap=50
)
```

递归字符分块的核心思想是采用一组有层次结构的分隔符（如段落、句子、单词）进行递归分割，旨在有效平衡语义完整性与块大小控制这两个核心目标。在  `RecursiveCharacterTextSplitter` 的实现中，它首先尝试使用最高优先级的分隔符（如段落标记）来切分文本。如果切分后的块仍然过大，该方法会继续对这个大块应用下一优先级分隔符（如句号），如此循环往复，直到块满足大小限制。这种分层处理的策略，使其在尽可能保持高级语义结构完整性的同时，又能有效控制块大小。

### 3.3 基于文档结构的分块

对于具有明确结构标记的文档格式（如Markdown、HTML、LaTex），可以利用这些标记来实现更智能、更符合逻辑的分割。

#### Markdown 结构分块

- **实现原理**: 根据Markdown的标题（`#`、`##`等）来分割文档，可以将一个章节或小节作为一个完整的块。
- **优势**: 完美保留文档的逻辑层次结构，语义内聚性极高。每个块都与一个特定的标题相关联，非常适合问答。
- **劣势**: 只适用于格式良好的Markdown文档。

## 四、高级分块策略

当基础策略无法满足复杂需求时，可以考虑以下更高级的技巧。

### 4.1 语义分块

- **实现原理**: 不依赖于固定大小或特定字符，而是使用语言模型来分析句子之间的语义相似性。当相邻句子之间的语义差异超过某个阈值时，就进行切分。
- **优势**: 能最精准地捕捉语义边界，生成的块在概念上是独立的、内聚的。
- **劣势**: 计算成本非常高，处理速度慢，需要调用额外的（通常是嵌入）模型来计算相似度。
- **代表库**: `semantic-text-splitter` (by LlamaIndex)

**适用场景**: 对检索质量要求极高，且不计较预处理成本的场景，如法律文书分析、科研论文检索。

### 4.2 块重叠

`chunk_overlap` 是一个看似简单却非常重要的参数。

- **目的**: 在相邻的两个块之间保留一部分重复的内容。这可以确保在块边界处的句子或概念不会被硬生生切断，从而保留了上下文的连续性。
- **如何选择**:
    - **太小**: 无法有效连接上下文。
    - **太大**: 增加存储和计算冗余，可能引入不必要的噪声。
    - **经验法则**: 通常设置为 `chunk_size` 的 **10% - 20%** 是一个不错的起点。例如，`chunk_size=512`，`chunk_overlap`可以设为`50`。

### 4.3 添加元数据

分块不仅仅是切割文本，更重要的是为每个块添加`身份证`——元数据。

- **常见元数据**:
    - `source`: 文档来源（文件名、URL）
    - `page_number`: 页码
    - `chapter`/`section`: 章节标题
    - `start_index`: 在原文中的起始位置

- **作用**:
    1.  **来源追溯**: 生成答案后，可以告诉用户答案来自哪个文档的哪个部分。
    2.  **过滤检索**: 在检索时可以先通过元数据进行过滤（例如，只在“第三章”中检索）。
    3.  **上下文丰富**: 将元数据信息（如标题）一并送给LLM，帮助其更好地理解块的内容。

## 五、如何选择分块策略

没有万能的策略，选择取决于你的数据和应用。

| 策略类型 | 适用数据 | 优点 | 缺点 | 推荐度 |
| :--- | :--- | :--- | :--- | :--- |
| **递归字符分块** | 通用文本，特别是半结构化文档 | **智能降级分割**，严格控制块大小，语义保护好 | 递归计算开销，分隔符需调优 | ★★★★★ (首选) |
| **Markdown/HTML分块** | 格式良好的Markdown、HTML文档 | **语义最完整**，逻辑清晰，自带元数据 | 适用范围窄 | ★★★★☆ (若适用) |
| **语义分块** | 任何文本，特别是段落密集的文章 | **理论上效果最好**，概念内聚 | 计算成本极高，速度慢 | ★★★☆☆ (特定场景) |
| **固定大小分块** | 结构化数据、代码、日志文件 | 速度最快，实现最简单 | **严重破坏语义**，上下文丢失 | ★☆☆☆☆ (不推荐) |

**决策流程建议**:

1.  **检查文档格式**:
    - 是不是格式规范的 Markdown 或 HTML？**是** -> 优先使用 `MarkdownHeaderTextSplitter`。
    - **否** -> 进入下一步。
2.  **评估性能要求**:
    - 是否对检索质量有极致要求，且能接受高昂的预处理成本？**是** -> 尝试**语义分块**。
    - **否** -> 进入下一步。
3.  **默认选择**:
    - 在绝大多数情况下，**`RecursiveCharacterTextSplitter`** 都是最稳妥、最高效的选择。从它开始，调整 `chunk_size` 和 `chunk_overlap` 进行实验。

最终，最佳的分块策略和参数组合需要通过在你的特定数据集上进行**评估和迭代**来确定。

## 六、分块结果可视化

理论讲了这么多，但如何直观地感受不同分块策略带来的差异呢？“眼见为实”是检验分块效果的最佳方式。通过可视化工具，我们可以清晰地看到：

-   文本是如何被切分的。
-   `chunk_overlap` 在哪里生效。
-   句子或段落是否被完整保留。

一个好用的社区工具是 [**ChunkViz**](https://github.com/FullStackRetrieval-com/chunkviz)。它可以将你的文档、分块配置作为输入，生成一个交互式的HTML文件，用不同的颜色块展示每个 chunk 的边界和重叠部分。

![ChunkViz 可视化示例](https://raw.githubusercontent.com/datawhalechina/llm-universe/main/docs/images/c2/2/chunkviz-demo.png)

通过这种可视化，你可以快速诊断出分块策略的问题（例如，一个重要的表格被无情地切开），并据此调整 `chunk_size`、`chunk_overlap` 或更换分块策略，从而为你的 RAG 系统打下坚实的基础。


## 参考文献

[^1]: `[CLS]` 是BERT等Transformer模型在输入文本开头添加的特殊标记，它通过自注意力机制动态聚合整个序列的上下文信息，其最终向量被训练用作代表全局语义的嵌入。

[^2]: Nelson F. Liu, et al. (2023). Lost in the Middle: How Language Models Use Long Contexts. [arXiv:2307.03172](https://arxiv.org/abs/2307.03172).
